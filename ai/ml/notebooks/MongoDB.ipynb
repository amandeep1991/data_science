{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databases contains collections which contains documents\n",
    "\n",
    "Format to access collection ---> <Database_name>.<Collection_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands\n",
    "\n",
    "help - to show commands\n",
    "\n",
    "db\n",
    "\n",
    "show dbs - show all available databases\n",
    "\n",
    "use <database_name> - switch database and hold the reference to db variable\n",
    "\n",
    "mongod -dbpath=F:\\Engineering\\installedTools\\MongoDB\\Server\\4.0\\data\n",
    "\n",
    "db.movies.insertOne({})-\n",
    "    insert a document in movies collection in the current db (if db is not created yet, it will create it first and then add document to a given collection)\n",
    "    it will return a document only (json) having information of acknowledgement and inserted id (_id is must for every document and if not provided mongo will create it by it's own)\n",
    "        By default index is created on _id field and we have to manually create other indexes on other columns.\n",
    "![12_MOngoDB_id.JPG](img/12_MOngoDB_id.JPG)\n",
    "    \n",
    "\n",
    "We can copy a record from compass, which doesn't show _id field in the copy prompt.\n",
    "\n",
    "#### Bulk Inserts - Ordered inserts [default - stop as and when it encounters error] or Unordered inserts \n",
    "db.movies.insertMany([{<jsonData1>}, {<jsonData2>}])\n",
    "            or\n",
    "db.movies.insertMany([{<jsonData1>}, {<jsonData2>}], {ordered:false})\n",
    "\n",
    "\n",
    "db.movies.find() \n",
    "    or \n",
    "db.movies.find().pretty() \n",
    "    or \n",
    "db.movies.find({}).pretty() \n",
    "    or \n",
    "db.movies.find({\"title\":\"Kick\"}).pretty() - \n",
    "    returns a cursor object not array. (hasNext & next methods)\n",
    "       - can even filter keys with given value in nested documents.\n",
    "\n",
    "\n",
    "show collections\n",
    "\n",
    "db.moviesScratch.insertOne({title:\"Star Trek II\", year:1982, imdb:\"tt0084726\"})\n",
    "\n",
    "\n",
    "db.moviesScratch.drop() # to drop this collection\n",
    "\n",
    "    \n",
    "Using Campus:\n",
    "    1. {\"birth year\": {\"$gte\":1985, \"$lt\":2001} }\n",
    "    \n",
    "    \n",
    "db.names.save - opens the javascript function defination\n",
    "\n",
    "var j = db.name.find() or db.name.findOne()\n",
    "j.name='Aman'\n",
    "db.name.save(j) - will update the value of cell in the tuple\n",
    "\n",
    "Handling Arrays:\n",
    "1) On the entire Array.\n",
    "2) Based on any element\n",
    "3) based on a specific element\n",
    "4) More Complex matches using operators\n",
    "db.movies.find({cast:'Jeff Bridges'}).pretty() # All documents will pass who has \"Jeff Bridges\" cast array (any position)\n",
    "db.movies.find({\"cast.0\":'Jeff Bridges'}).pretty() # All documents will pass who has \"Jeff Bridges\" in cast array at index 0 [Main actor of movie]\n",
    "db.movies.find({cast:['Jeff Bridges', 'Tim Robbins']}).pretty() # will search exact match (size, order of elements both are important)\n",
    "\n",
    "db.movies.find().pretty().count() # would return count even if we remove pretty from between.\n",
    "\n",
    "\n",
    "\n",
    "### Cursors: (can also override the batch size)\n",
    "MongoDB Server returns results to it's clients in batches (batch size would be 101 documents or documents having less than or equal to 1 megabyte data - subsequent batches would be 4 MB). By default shell iterates the cursor 20 times if response not stored in variable.\n",
    "var c = db.movieDetails.find()\n",
    "var doc= function() {return c.hasNext() ? c.next(): null;}\n",
    "c.objsLeftInBatch(); # to check how many objects are in the batch\n",
    "now repeatedly call doc() functions to iterate.\n",
    "now call c.objsLeftInBatch() again and the count would be reduced now.\n",
    "\n",
    "### Projections:\n",
    "Avoid network over-head and processing requirement etc. by limiting the fields.\n",
    "db.movieDetails.find({\"genres.1\":\"Western\"},{title:1}) # title + id (by default)\n",
    "db.movieDetails.find({\"genres.1\":\"Western\"},{title:1, _id:0}) # only title \n",
    "0 means exclude\n",
    "1 means include\n",
    "So if we just need to remove 2 fields out of 1000s then simply mention these 2 fields with 0 value in second parameter.\n",
    "\n",
    "### Update (updateOne will update the 1st it encounters) - can be used for nested fields as well\n",
    "db.movieDetails.updateOne({title:\"El Topo\"}, {$set: {poster1:\"AmandeepAddedThis\"}}) # 1st is filter, 2nd is setter\n",
    "Update Operators: $addToSet, $pullAll, $each etc for arrays and $inc, $set, $unset etc for other scenarios.\n",
    "db.movieDetails.updateOne( {title: \"The Martian\"}, \n",
    "{$push: {reviews: { #push would make sure that \n",
    "    $each: [  # it's like flatArray in Java 8 stream, if not applied it takes complete array as one element and push it in the document\n",
    "                {\n",
    "                    rating: 0.5,\n",
    "                    date: ISODate(2016-06-12T07:00:00Z),\n",
    "                    reviewer: \"Shannon B.\",\n",
    "                    text: \"Enjoyed watching with my kids!\"\n",
    "                }\n",
    "            ],\n",
    "     $slice: 5 # to keep only 5 records(discards old ones),\n",
    "     $position: 0 # to push in the very starting of the array\n",
    "    \n",
    "}}} )\n",
    "\n",
    "\n",
    "db.movieDetails.updateMany({rating:null}, {$unset:{rating:\"\"}}) # \"\" value doesn't matter here because operator is unset.\n",
    "\n",
    "UpSerts: (update query which inserts the value if no value is found)\n",
    "detail = db.movieDetails.findOne({\"imdb.id\":\"tt4368814\"})\n",
    "db.movieDetails.updateOne(\n",
    "    {\n",
    "        \"imdb.id\":detail.imdb.id\n",
    "    }, \n",
    "    {\n",
    "        $set: detail\n",
    "    },\n",
    "    {\n",
    "        upsert:true\n",
    "    }\n",
    ") # this query ensures that it wouldn't create a separate document, just one document having this imdb.id.\n",
    "\n",
    "\n",
    "#### To replace an entire document\n",
    "db.movieDetails.replaceOne({<filter>}, <newDocument>)\n",
    "#why: you can save an object in python or javascript program and make hell lot of changes and then give it to server/mongodb to just replace it. If upsert flag is true, it just save new element (in case of update_one or update_many, it adds the filter as well)\n",
    "\n",
    "\n",
    "# as for updates\n",
    "db.movieDetails.deleteOne()\n",
    "db.movieDetails.deleteMany()\n",
    "\n",
    "\n",
    "### Query selectors\n",
    "#### Comparision: $eq, $gt, $gte, $lt, $lte, $in, $nin\n",
    "db.movieDetails.find({runtime: {$gt:90} } )\n",
    "db.movieDetails.find({runtime: {$gt:90, $lte:120} }, {title:1} )\n",
    "db.movieDetails.find({runtime: {$gt:90, $lte:120}, rated:{$ne: \"UNRATED\"} }, {title:1} )\n",
    "db.movieDetails.find({rated:{$in: [\"UNRATED\", \"PG\"]} }, {title:1} )\n",
    "\n",
    "#### Element: $exists, $type\n",
    "db.movieDetails.find({\"tomoto.meter\": {$exists:true} } )\n",
    "db.moviesDetails.find({\"_id\": {$type:\"string\"}}) # used in case same field have different types, but ideally it should not be the case.\n",
    "\n",
    "Now consider a scenario where for some values of a key, we have value stored as null (without quotes) and we have few other records who doesn't have any such key. Now we want to make a query which fetches both of these records. To cater this we can use following query:\n",
    "db.movieDetails.find({\"tomoto.meter\":null})\n",
    "\n",
    "#### Logical: $or (takes array), $and, $not, $nor\n",
    "db.movieDetails.find({ $or: [{\"tomoto.meter\": {$exists:true} }, {\"metacritic\": {$gt: 88}}]} )\n",
    "Ok That's great about OR operator but why we need AND operator, after all it is equivalent to what we give in simple find method:\n",
    "Actually in simple find method call, we can't specify the same key more than once (as the 'where' clause is actually a json, so last element will override the duplicate key) but that can be achieve as shown below:-\n",
    "db.movieDetails.find({ $and: [ {\"metacritic\": {$ne: null}}, {\"metacritic\":{$exists:true}} ] })\n",
    "\n",
    "#### Regex Operator: $regex (/slashes/ signifies the beginning and end of the regex expression)\n",
    "db.movieDetails.find( { \"awards.text\": {$regex: /^Won.*/} } )\n",
    "db.movieDetails.find( { \"awards.text\": {$regex: /^Won\\s.*/} } )\n",
    "^ means very beginning of text we want to match the next character (next to ^) = case-sensitive\n",
    ". = match any character\n",
    "* = any number of times of previous character\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Array Operator: $all, $size, $elemMatch\n",
    "db.movieDetails.find( { genres: {$all: [\"Comedy\", \"Crime\"]} } ) # all exist in queried array\n",
    "db.movieDetails.find( {countries: {$size: 1} } )\n",
    "For scenarios where there is an array of json objects and we want to satisfy 2 conditions for every json element like below:-\n",
    "Data = boxOffice: [ \n",
    "                    {\"country\": \"USA\", \"revenue\":41.3} ,\n",
    "                    {\"country\": \"Australia\", \"revenue\":2.9} ,\n",
    "                    {\"country\": \"UK\", \"revenue\":10.1} ,\n",
    "                    {\"country\": \"Germany\", \"revenue\":4.3} ,\n",
    "                  ]\n",
    "FirstTry =  db.movieDetails.find({boxOffice: {country:\"UK\", revenue:{$gt:15}}})\n",
    "SecondTry =  db.movieDetails.find({boxOffice.country:\"UK\", boxOffice.revenue:{$gt:15}})\n",
    "\n",
    "CorrectTry = db.movieDetails.find({boxOffice: {$elematch { country:\"UK\", revenue: {$gt:15} }}})\n",
    "\n",
    "\n",
    "Run JS via mongo Shell:\n",
    "mongo < myJavascript.js\n",
    "\n",
    "\n",
    "type reddit.json | python -m json.tool\n",
    "\n",
    "\n",
    "\n",
    "> db.movieDetails.find({$or: [    { 'awards.wins':{$gt:0} }  ,  {'awards.oscars':{$size: {$gt:0} } }  ]})\n",
    "Error: error: {\n",
    "        \"ok\" : 0,\n",
    "        \"errmsg\" : \"$size needs a number\",\n",
    "        \"code\" : 2,\n",
    "        \"codeName\" : \"BadValue\"\n",
    "}\n",
    "db.movieDetails.find({$or: [    { 'awards.wins':{$gt:0} }  ,  {'awards.oscars.0':{$exists: true } }  ]})\n",
    "\n",
    "\n",
    "\n",
    "db.movieDetails.updateMany({ year: {$gte: 2010, $lte: 2013}, \"imdb.votes\": {$lt: 10000},\"tomato.consensus\": null },\n",
    "                           { $unset: { \"tomato.consensus\": \"\" } });\n",
    "                           \n",
    "db.movieDetails.updateMany({ year: {$gte: 2010, $lte: 2013}, \"imdb.votes\": {$lt: 10000},\"tomato.consensus\": null }, \n",
    "                           { $unset: { \"tomato.censensus\": \"\" } });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo DB schema Design\n",
    "\n",
    "Although mongo-db is schema-less, but if we don't look much at schema it will impact the performance.\n",
    "\n",
    "Now talking about the relational database, we know that during application development for schema design, we try to keep all tables atleast at 3rd-Normal form. In Mongo-DB we can keep data in 3rd-Normal Form.\n",
    "\n",
    "But in mongodb, we keep schema considering the principle of \"Application Driven Schema\". We think about:\n",
    "* What pieces of data are used together.\n",
    "* What pieces of data are mostly read-only.\n",
    "\n",
    "Accordingly we design the mongodb schema to suit the application data access patterns. We try to keep data in a way that is agnostic to the application.\n",
    "\n",
    "Basic Facts of MongoDB:\n",
    "* supports rich documents (meaning not only tabular data, but arrays of values, arrays of documents, nested documents etc)\n",
    "* allows pre-join of data/ embed data\n",
    "* Doesn't support join directly inside the kernel (if we need this we need to join in the application itself)\n",
    "    * the reason is that joins are hard to scale.\n",
    "* No constraints (not as important due to embedding)\n",
    "* Don't support transactions but do support atomicity (we would discuss how to design our schema in order to support atomicity)\n",
    "* No declared schema, but there is a good chance that your application is going to have a schema. \n",
    "\n",
    "\n",
    "\n",
    "Goals of normalization in relational databases:\n",
    "* Free the database from modification anamolies. # For MongoDB we would be careful not to create these embeded documents, occasionally we might have a trade-off between redundancy and performance and then we might give priority to performance and maintain embedded documents.\n",
    "* Minimize the redesign while extending the database.\n",
    "* Avoid any bias toward any particular access pattern. # this would be avoided in MongoDB\n",
    "\n",
    "Alternative schema design for blog application: (separate seek time for each different type of document - so pre-join)\n",
    "        ![12_MOngoDB_id_2.JPG](img/12_MOngoDB_id_2.JPG)\n",
    "        \n",
    "\n",
    "Living without constraints:\n",
    "* consistency is actually guranteed by constraints in RDBMS\n",
    "* But in MongoDB, there is no such gurantee. This would be the responsibility of developer. However MongoDB embedded concept helps a bit to decrease the problem of not having constraints.\n",
    "\n",
    "\n",
    "Living without transactions(ACID):\n",
    "* But we do have atomic operation.\n",
    "* Steps to take advantage of atomic operations:\n",
    "    1. Restructure - so that you are working on a single document. If you do that then you are all set.\n",
    "    2. Implement in software - like if we want to transfer some amount from one bank to another, then there couldn't be any transaction support across different RDBMS but the same problem/issue is resolved by software/application.To\n",
    "    3. Tolerate a little bit of inconsistency - It is ok if one person sees a friend feed a little late (or not updated real-time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One to One Relations:\n",
    "* Examples:\n",
    "    * Employee - Resume\n",
    "    * Building - FloorPlan\n",
    "    * Patient - Medical History\n",
    "* Things to consider to decide the schema:\n",
    "    * Frequency of access - (whether you access more of Employee data or Resume data, if employee are getting accessed more then is it ok to load all resume data if embeded, if embeded which one should be put inside to other depends upon access frequency)\n",
    "    * Which of these are growing all the time or not growing:\n",
    "        * One of the reason to keep these separate is that resume might be getting updated frequently and so you don't want to load employee details whenever there is an addition of employer in the resume.\n",
    "        * And also if the size of resume is >16 MB then you might not be able to embed the employee in it.\n",
    "    * Atomicity of the document - If you want to update both employee and resume details atomically then put that in single document.\n",
    "    \n",
    "    \n",
    "Many to One Relations:\n",
    "* Examples:\n",
    "    * city - persons (new york city - 8 million people)\n",
    "* Discussion for schema:\n",
    "    * We ofcourse can't  embed person documents in city as there could be many persons and it would exceed that 16 MB condition.\n",
    "    * Now tring to put city inside each person, but then there would be a lot of data redundancy which may lead to consistency issues.\n",
    "* Solution - TRUE LINKING:\n",
    "    - 2 collections (people[name, myCity] & city[city_id, <other details>]) - consistency would be guaranteed by application code.\n",
    "    - But if we have one to few relations:\n",
    "        * put the many embedded in the oneth relation.\n",
    "        \n",
    "When is it recommended to represent a one to many relationship in multiple collections?\n",
    "    Whenever many is large\n",
    "    \n",
    "    \n",
    "Many to Many Relations:\n",
    "* Examples:\n",
    "    * Books - Authors: [Few to Few]\n",
    "    * Students - Teachers:\n",
    "* Discussion for schema:-\n",
    "    * Few to Few: \n",
    "        - 2 collections with references to both collections (as both are first level access objects)\n",
    "        - embed books under author (avoid it unless performance constraint)\n",
    "    * Many to few:\n",
    "        - since teachers are generally few and they are actually required first so we create them alone\n",
    "        - and in students document we add linking for teachers\n",
    "        \n",
    "\n",
    "Benifits of embedding:\n",
    "    * Performance - proof-read performance (one trip to the DB)\n",
    "    * General process of reading - it usually takes a lot of time to read the first byte and thereafter it reads every next byte pretty quickly.\n",
    "\n",
    "\n",
    "Trees:\n",
    "    * out of many db design problems, one is how you design underlying storage structure of tree type of data.\n",
    "    * In MongoDB, we can define children/ancestors etc.\n",
    "    \n",
    "    \n",
    "When to denormalize?\n",
    "    * As we have seen, mongodb prefer embedded documents for performance point of view. It seems like we are denormalizing the data.\n",
    "    * But till the point where we are duplicate the data, we don't open ourself to modification anamolies.\n",
    "    \n",
    "    \n",
    "Handling blobs:\n",
    "    - Storing large files:\n",
    "        * by default 16 M.B. is the maximum storage size (document size limit)\n",
    "        * But MongoDB also provide a special facility called GRIDFS that will break up large file into chunks and store those chunks into the collections and also store metadata about these chunks in secondary collection.\n",
    "        * Say suppose we have a 100 MB video and we want to store the same in MongoDB\n",
    "                gridfs.GridFS(db, \"video.mp4\")\n",
    "        \n",
    "        * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharding:\n",
    "\n",
    "is a type of database partitioning that separates very large databases the into smaller, faster, more easily managed parts called data shards\n",
    "\n",
    "data shards can be distributed across a number of much less expensive commodity servers. Data shards have comparatively little restriction as far as hardware and software requirements are concerned.\n",
    "\n",
    "In some cases, database sharding can be done fairly simply. One common example is splitting a customer database geographically.\n",
    "\n",
    "Data sharding can be a more complex process in some scenarios, however. Sharding a database that holds less structured data, for example, can be very complicated, and the resulting shards may be difficult to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale-up = increasing the power of CPU (scale vertical)\n",
    "\n",
    "Scale-out = increasing the number of CPUs (scale horizontally) - MongoDB natively supports Scale-out concept (as no join is needed)\n",
    "\n",
    "MongoDB also supports agile methodologies and performance needs of modern application.\n",
    "\n",
    "MongoDB University course uses following things:\n",
    "    1. MongoD (Database) - starting the server, use mongod and before that make sure to create directory \\data\\db\n",
    "    2. Mongo (Shell) - when we run it, it makes connection to local database - now can run query on this prompt\n",
    "    3. HTTP server (python, pymongo(mongo driver for python), bottle(web-framework) etc)\n",
    "    \n",
    "    \n",
    "Datatypes supported in JSON - string,int,boolean,array,objects(nested jsons)\n",
    "\n",
    "BSON - MongoDB stores data as BSON (Binary JSON) - This is the communication medium between mongoDB driver (at application side) and MOngoDB server. Below are the advantages of BSON-\n",
    "    Light weight\n",
    "    Traversable\n",
    "    Efficient\n",
    "    \n",
    "\n",
    "Limitations of JSON: - BSON removed this limitations\n",
    "    No support for date objects - must be stored as strings or numbers\n",
    "    No difference between integers or float - all are stored as numbers\n",
    "    \n",
    "\n",
    "Indexing can be done on fields/keys with value of Array type as well.\n",
    "\n",
    "Keys must be strings\n",
    "\n",
    "MongoDB is schemaless, so if no value exist for a given key simply leave it.\n",
    "\n",
    "Document size can't be more than 16 MB\n",
    "\n",
    "Python Dict vs JSON:\n",
    "    1. Ordering is maintained in JSON not in dict.\n",
    "    2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottle\n",
    "\n",
    "bottle.template(\"hello_world\", {\"key1\":\"value1\"}) # \"hello_world.tpl\" is the template file name and {{variable_name}} to populate the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Performance of any computer application is derived by many things like:\n",
    "1. Underlying hardware (CPU, memory, disk etc)\n",
    "2. After hardware, algorithms comes into picture to determine the performance\n",
    "3. And for database backed application, it is going to be the algorithms that satisfies queries.\n",
    "\n",
    "There are two ways you can impact **latency and throughput** of database queries:\n",
    "1. Add indexes to the collections.\n",
    "2. Distribute the load across multiple servers using sharding.\n",
    "\n",
    "Pluggable storage engines added in MOngoDB 3 (controls how data is going to store on disk)\n",
    "\n",
    "Now database performance is actually on the plate of database administrators, but can developers deep dive into it and write their code keeping database performance in mind and they can debug the problems related to the performance and fix it. \n",
    "\n",
    "Pluggable storage engines: an interface between persistence storage (disks) and mongoDB\n",
    "    * Flow - \n",
    "        ** python code interact with mongodb using pymongo where updation,deletion, insertion extra happens\n",
    "        ** mongodb pass it to the pluggable storage engine\n",
    "        ** which then passes it finally to the disk\n",
    "        \n",
    "        ** Now there could be case the storage engine would like to use memory to optimize the process. In other words the disk is quite slow\n",
    "        \n",
    "        ** So we introduce the pluggable concept and depending upon the type of storage engine plugged in the application, we get the performance characteristics.\n",
    "        \n",
    "   * Two storage engines shipped with mongodb by default:\n",
    "       ** MMAP (default)\n",
    "       ** WiredTiger - acquisition happened in 2014\n",
    "       \n",
    "   * what storage engine don't do?\n",
    "       ** It doesn't have any control/impact on the communication between different mongodb servers in a cluster.\n",
    "       ** Neither it has any impact on the APIs exposed to the python developer (or any other language developer)\n",
    "    \n",
    "  * The storage engine directly determines which of the following? Check all that apply.\n",
    "      ** The data file format  (answer)\n",
    "      ** Architecture of the cluster\n",
    "      ** The wire protocol of drivers\n",
    "      ** Format of indexes (answer)\n",
    "      \n",
    "      \n",
    "* MMAP\n",
    "    ** Also called version 1 as mongodb starts with this\n",
    "    ** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
