{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the curse of dimensionality? Can you list some ways to deal with it? - 4 methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality is when the training data has a high feature count, but the dataset does not have enough samples for a model to learn correctly from so many features. For example, a training dataset of 100 samples with 100 features will be very hard to learn from because the model will find random relations between the features and the target. However, if we had a dataset of 100k samples with 100 features, the model could probably learn the correct relationships between the features and the target.\n",
    "\n",
    "There are different options to fight the curse of dimensionality:\n",
    "\n",
    "    1. Feature selection: Instead of using all the features, we can train on a smaller subset of features.\n",
    "    2. Dimensionality reduction: There are many techniques that allow to reduce the dimensionality of the features. Principal component analysis (PCA) and using autoencoders are examples of dimensionality reduction techniques.\n",
    "    3. L1 regularization: Because it produces sparse parameters, L1 helps to deal with high-dimensionality input.\n",
    "    4. Feature engineering: Itâ€™s possible to create new features that sum up multiple existing features. For example, we can get statistics such as the mean or median."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
