{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s the trade-off between bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is error due to erroneous or over-simplistic assumptions of the Model, like Linear Model has assumption that data would following a linear curve. Can lead to underfitting. High bias can cause an algorithm to miss the relevant relations between features and target outputs.\n",
    "\n",
    "\n",
    "Variance is error due to too much complexity of learning algorithm (algo highly sensitive to the variations in data). Can lead to overfitting. High variance can cause an algorithm to model the random noise in the training data.\n",
    "\n",
    "learning error = =bias + variance + irreducible error due to noise \n",
    "\n",
    "If you make the model more complex and add more variables, you’ll lose bias but gain some variance — in order to get the optimally reduced amount of error, you’ll have to tradeoff bias and variance. You don’t want either high bias or high variance in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain how a ROC curve works. (Sensitivity vs Fall-out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. \n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "                    True Positive Rate\n",
    "                    False Positive Rate\n",
    "\n",
    "It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).\n",
    "\n",
    "Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n",
    "\n",
    "Sensitivity = recall or probability of detection\n",
    "fall-out = probability of false alarm (1-specificity)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.\n",
    "\n",
    "AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
    "\n",
    "AUC is desirable for the following two reasons:\n",
    "    1. AUC is scale-invariant: \n",
    "        1. It measures how well predictions are ranked, rather than their absolute values.\n",
    "        2. However, scale-invariance is not always desirable, sometimes we really do need well calibrated probability outputs and AUC won't tell us about that.\n",
    "        \n",
    "    2. AUC is classification-threshold-invariant: \n",
    "        1. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "        2. Classification-threshold invariance is not always desirable. In cases where there are **wide disparities** in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is more important to you– model accuracy, or model performance? [accuracy paradox]\n",
    "\n",
    "The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall.\n",
    "\n",
    "Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
