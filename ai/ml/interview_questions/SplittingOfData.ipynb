{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need a validation set and test set? What is the difference between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three types of data-sets:\n",
    "\n",
    "1. The training dataset:  for fitting the model’s parameters can't rely for the accuracy on unseen data.\n",
    "\n",
    "2. The validation dataset: \n",
    "    1. measures performance of data not part of training set. \n",
    "    2. to tune the hyperparameters of trained model. \n",
    "    3. Not reliable, as every time we evaluate the validation data and we make decisions based on those scores, we are leaking information from the validation data into our model. The more evaluations, the more information is leaked. So we can end up overfitting to the validation data, and once again the validation score won’t be reliable for predicting the behaviour of the model in the real world.\n",
    "    \n",
    "    \n",
    "3. The test dataset: \n",
    "    1. measure how well the model does on previously unseen examples. \n",
    "    2. should be used after parameters tuning using the validation set.\n",
    "\n",
    "So if we omit the test set and only use a validation set, the validation score won’t be a good estimate of the generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "One of the common practices to avoid overfitting is to hold onto part of the available data and use it as a test set. However, when evaluating different model settings, such as the number of polynomial features, we are still at risk of overfitting the test set because parameters can be tweaked to achieve the optimal estimator performance and, because of that, our knowledge about the test set can leak into the model. To solve this problem, we need to hold onto one more part of the dataset, which is called the “validation set.” Training proceeds on the training set and, when we think that we’ve achieved the optimal model performance, we can make a final evaluation utilizing the validation set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we dramatically reduce the number of samples that can be used for training the models, and the results can depend on a particular random choice for the training-validation pair of sets.\n",
    "\n",
    "One solution to this problem is a procedure called cross-validation. In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k−1 folds while using the remaining fold as the test set (called the “holdout fold”).\n",
    "\n",
    "Cross-validation allows you to tune parameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
    "\n",
    "There are a lot more cross-validation techniques, like:\n",
    "#### leave P out, \n",
    "#### stratified k-fold, \n",
    "#### shuffle and split, etc. \n",
    "\n",
    "\n",
    "![9_Holdout_crossvalidation.jpg](img/9_Holdout_crossvalidation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is stratified cross-validation and when should we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique for dividing data between training and validation sets. Generally it is random but in stratified,the split preserves the ratio of the categories on both the training and validation datasets.\n",
    "\n",
    "For example, if we have a dataset with 10% of category A and 90% of category B, and we use stratified cross-validation, we will have the same proportions in training and validation. In contrast, if we use simple cross-validation, in the worst case we may find that there are no samples of category A in the validation set.\n",
    "\n",
    "Stratified cross-validation may be applied in the following scenarios:\n",
    "    1. On a dataset with multiple categories. The smaller the dataset and the more imbalanced the categories, the more important it will be to use stratified cross-validation.\n",
    "    2.  On a dataset with data of different distributions. For example, in a dataset for autonomous driving, we may have images taken during the day and at night. If we do not ensure that both types are present in training and validation, we will have generalization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What cross-validation technique would you use on a time series dataset?\n",
    "Instead of using standard k-folds cross-validation, you have to pay attention to the fact that a time series is not randomly distributed data — it is inherently ordered by chronological order. If a pattern emerges in later time periods for example, your model may still pick up on it even if that effect doesn’t hold in earlier years!\n",
    "\n",
    "Time-series (or other intrinsically ordered data) can be problematic for cross-validation. If some pattern emerges in year 3 and stays for years 4-6, then your model can pick up on it, even though it wasn't part of years 1 & 2.\n",
    "\n",
    "An approach that's sometimes more principled for time series is forward chaining (rolling concept), where your procedure would be something like this:\n",
    "    fold 1 : training [1], test [2]\n",
    "    fold 2 : training [1 2], test [3]\n",
    "    fold 3 : training [1 2 3], test [4]\n",
    "    fold 4 : training [1 2 3 4], test [5]\n",
    "    fold 5 : training [1 2 3 4 5], test [6]\n",
    "    \n",
    "That more accurately models the situation you'll see at prediction time, where you'll model on past data and predict on forward-looking data. It also will give you a sense of the dependence of your modeling on data size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
