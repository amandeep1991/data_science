{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Advantages:\n",
    "\n",
    "1. Readable\n",
    "2. Garbage Collected\n",
    "3. Dynamically Typed\n",
    "\n",
    "Important Topics\n",
    "    tracking Metrics & success metrics\n",
    "    A/B testing (back-testing)\n",
    "    \n",
    "    \n",
    "Most machine learning algorithms work by finding a statistical dependency in the data provided to them. This dependency is called a hypothesis and is usually denoted by h(θ)\n",
    "\n",
    "Why we use sigmoid function in Logistic Regression?\n",
    "    Using a sigmoid function, we can convert any numerical value to represent a value on the interval [−1,1].\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Machine Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Reasons:\n",
    "1. Reduce Programming Time\n",
    "2. Customize and scale products - English language spectator but moving to another language would require a lot of effort\n",
    "3. Complete seemingly \"unprogrammable\" tasks: \n",
    "\n",
    "\n",
    "Physological Reasons:\n",
    "1. Different Thinking (Mathematical Science -> Natural Science)\n",
    "2. Using statistics not logic to analyze a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a method for finding the straight line or hyperplane that best fits a set of points.\n",
    "\n",
    "L2 Loss = Square Loss\n",
    "\n",
    "Mean square error (MSE) = 1/N sum((y-prediction) ** 2)\n",
    "\n",
    "\n",
    "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Machine Learning model is trained by starting with an initial guess for the weights and bias and iteratively adjusting those guesses until learning the weights and bias with the lowest possible loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define precision and recall\n",
    "\n",
    "Precision is the fraction of retrieved documents that are relevant to the query.\n",
    "Recall is the fraction of the relevant documents that are successfully retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## static and dynamic training\n",
    "\n",
    "Static:\n",
    "    1. Advantages\n",
    "        1. Easy to build and test -- use batch train & test, iterate until good.\n",
    "    2. Disadvantages\n",
    "        1. Still requires monitoring of inputs (validation that data format/type hasn't change/evolved)\n",
    "        2. Easy to let this grow stale\n",
    "    3. Usages:\n",
    "        1. When we know that data is not going to change.\n",
    "        2. Like image recognisation model, we know data is not going to change that often\n",
    " \n",
    "Dynamic:\n",
    "    1. Advantages:\n",
    "        1. Continue to feed in training data over time, regularly sync out updated version.\n",
    "        2. Use progressive validation rather than batch training & test\n",
    "        Will adapt to changes, staleness issues avoided\n",
    "    2. Disadvantages:\n",
    "        1. Needs monitoring, model rollback & data quarantine capabilities\n",
    "        2. Higher complexity of the system\n",
    "    3. Usage:\n",
    "        1. Analysis of trends or sentiments which can be changing frequently.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Bayes’ Theorem? How is it useful in a machine learning context?\n",
    "\n",
    "Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge.\n",
    "\n",
    "Mathematically, it’s expressed as the true positive rate of a condition sample divided by the sum of the false positive rate of the population and the true positive rate of a condition. \n",
    "\n",
    "Bayes’ Theorem is the basis behind a branch of machine learning that most notably includes the Naive Bayes classifier. \n",
    "\n",
    "\n",
    "\n",
    "#### Why is “Naive” Bayes naive?\n",
    "Despite its practical applications, especially in text mining, Naive Bayes is considered “Naive” because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features — a condition probably never met in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What’s the difference between Type I and Type II error?\n",
    "\n",
    "Type I error is a false positive, while Type II error is a false negative.\n",
    "\n",
    "Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.\n",
    "\n",
    "A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What’s a Fourier transform?\n",
    "\n",
    "Here's a plain-English metaphor:\n",
    "    1. What does the Fourier Transform do? Given a smoothie, it finds the recipe.\n",
    "    2. How? Run the smoothie through filters to extract each ingredient.\n",
    "    3. Why? Recipes are easier to analyze, compare, and modify than the smoothie itself.\n",
    "    4. How do we get the smoothie back? Blend the ingredients.\n",
    "    \n",
    "The Fourier Transform takes a time-based pattern, measures every possible cycle, and returns the overall \"cycle recipe\" (the amplitude, offset, & rotation speed for every cycle that was found).\n",
    "\n",
    "https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s the difference between a generative and discriminative model?\n",
    "\n",
    "A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.\n",
    "\n",
    "Let's say you have input data x and you want to classify the data into labels y. A generative model learns the joint probability distribution p(x,y) and a discriminative model learns the conditional probability distribution p(y|x) - which you should read as \"the probability of y given x\".\n",
    "\n",
    "Here's a really simple example. Suppose you have the following data in the form (x,y):\n",
    "(1,0), (1,0), (2,0), (2, 1)\n",
    "\n",
    "p(x,y) is:\n",
    "\n",
    "|     | y=0  |  y=1 |\n",
    "|-----|------|------|     \n",
    "| x=1 | 1/2  |   0  |\n",
    "| x=2 | 1/4  |  1/4 |\n",
    "\n",
    "\n",
    "\n",
    "p(x|y) is:\n",
    "\n",
    "|     | y=0  |  y=1 |\n",
    "|-----|------|------|     \n",
    "| x=1 |  1   |   0  |\n",
    "| x=2 | 1/2  |  1/2 |\n",
    "\n",
    "The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model p(x,y), which can be transformed into p(y|x) by applying Bayes rule and then used for classification. However, the distribution p(x,y) can also be used for other purposes. For example, you could use p(x,y) to generate likely (x,y) pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid over-fitting?\n",
    "\n",
    "There are three main methods to avoid overfitting:\n",
    "1. Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.\n",
    "2. Use cross-validation techniques such as k-folds cross-validation.\n",
    "3. Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
