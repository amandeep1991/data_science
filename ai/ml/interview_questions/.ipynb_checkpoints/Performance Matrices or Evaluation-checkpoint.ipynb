{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing and Comparing Classifier Performance with ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Metrics (the percent of correct classifications obtained): is good but it ignores many important factors\n",
    "\n",
    "Consider an example where 1% of patients have cancer and model simply return that \"Patient don't have cancer\". Then this model is 99% accurate but it is not reliable at all.\n",
    "\n",
    "Most classifiers produce a score, which is then thresholded to decide the classification. If a classifier produces a score between 0.0 (definitely negative) and 1.0 (definitely positive), it is common to consider anything over 0.5 as positive.\n",
    "However, any threshold applied to a dataset (in which PP is the positive population and NP is the negative population) is going to produce true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN) (Figure 1). We need a method which will take into account all of these numbers.\n",
    "\n",
    "Once you have numbers for all of these measures, some useful metrics can be calculated.\n",
    "    1. Accuracy = (1 – Error) = (TP + TN)/(PP + NP) = Pr(C), the probability of a correct classification.\n",
    "    \n",
    "    2. Sensitivity = TP/(TP + FN) = TP/PP = the ability of the test to detect disease in a population of diseased individuals.\n",
    "    \n",
    "    3. Specificity = TN/(TN + FP) = TN / NP = the ability of the test to correctly rule out the disease in a disease-free population.\n",
    "    \n",
    "\n",
    "By considering our wrong results as well as our correct ones we get much greater insight into the performance of the classifier.\n",
    "\n",
    "TP (sensitivity) can then be plotted against FP (1 – specificity) for each threshold used. The resulting graph is called a Receiver Operating Characteristic (ROC) curve. \n",
    "\n",
    "#### For a perfect classifier the ROC curve will go straight up the Y axis and then along the X axis. A classifier with no power will sit on the diagonal, whilst most classifiers fall somewhere in between.\n",
    "\n",
    "\n",
    "## Uses of ROC curve:\n",
    "    1. Threshold Selection:\n",
    "        It is immediately apparent that a ROC curve can be used to select a threshold for a classifier which maximises the true positives, while minimising the false positives.\n",
    "\n",
    "        However, different types of problems have different optimal classifier thresholds. For a cancer screening test, for example, we may be prepared to put up with a relatively high false positive rate in order to get a high true positive,  it is most important to identify possible cancer sufferers.\n",
    "\n",
    "        For a follow-up test after treatment, however, a different threshold might be more desirable, since we want to minimise false negatives, we don’t want to tell a patient they’re clear if this is not actually the case.\n",
    "        \n",
    "    2. Performance Assessment: (area under the curve (AUC))\n",
    "        ROC curves also give us the ability to assess the performance of the classifier over its entire operating range. The most widely-used measure is the area under the curve (AUC). The AUC for a classifier with no power (diagnol line), essentially random guessing, is 0.5, because the curve follows the diagonal. The AUC for that mythical being, the perfect classifier, is 1.0. Most classifiers have AUCs that fall somewhere between these two values.\n",
    "\n",
    "        An AUC of less than 0.5 might indicate that something interesting is happening. A very low AUC might indicate that the problem has been set up wrongly, the classifier is finding a relationship in the data which is, essentially, the opposite of that expected. In such a case, inspection of the entire ROC curve might give some clues as to what is going on: have the positives and negatives been mislabelled?\n",
    "        \n",
    "        \n",
    "    3. Classifiers Comparision:\n",
    "        The AUC can be used to compare the performance of two or more classifiers. A single threshold can be selected and the classifiers’ performance at that point compared, or the overall performance can be compared by considering the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s the F1 score? How would you use it?\n",
    "\n",
    "It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst.\n",
    "\n",
    "You would use it in classification tests where true negatives don’t matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What evaluation approaches would you work to gauge the effectiveness of a machine learning model?\n",
    "\n",
    "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n",
    "https://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/\n",
    "\n",
    "\n",
    "### How would you evaluate a logistic regression model?\n",
    "https://stats.stackexchange.com/questions/71517/evaluating-a-logistic-regression-model#71522\n",
    "\n",
    "\n",
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
