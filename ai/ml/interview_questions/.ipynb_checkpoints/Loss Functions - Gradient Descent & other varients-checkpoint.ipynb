{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss:\n",
    "\n",
    "1. Hyperparameters are the configuration settings used to tune how the model is trained.\n",
    "2. Derivative of (y - y')2 with respect to the weights and biases tells us how loss changes for a given example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goldilocks principle\n",
    "\n",
    "The Goldilocks principle is named by analogy to the children's story The Three Bears, in which a little girl named Goldilocks tastes three different bowls of porridge, and she finds that she prefers porridge which is neither too hot nor too cold, but has just the right temperature.[1] Since the children's story is well known across cultures, the concept of \"just the right amount\" is easily understood and is easily applied to a wide range of disciplines, including developmental psychology, biology,[2] astronomy, economics and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "\n",
    "    Weight Initialization:\n",
    "    \n",
    "        For convex problems, weights can start anywhere (say, all 0s)\n",
    "            Convex: think of a bowl shape\n",
    "            Just one minimum\n",
    "            \n",
    "        Foreshadowing: not true for **neural nets**\n",
    "            Non-convex: think of an egg crate\n",
    "            More than one minimum\n",
    "            Strong dependency on initial values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD & Mini-Batch Gradient Descent\n",
    "\n",
    "    1. Could compute gradient over entire data set on each step, but this turns out to be unnecessary\n",
    "    \n",
    "    2. Computing gradient on small data samples works well\n",
    "        1. On every step, get a new random sample\n",
    "        \n",
    "    3. Varients of Gradient Descent:\n",
    "        1. Stochastic Gradient Descent: one example at a time\n",
    "        2. Mini-Batch Gradient Descent: batches of 10-1000\n",
    "        \n",
    "    4. Loss & gradients are averaged over the batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
