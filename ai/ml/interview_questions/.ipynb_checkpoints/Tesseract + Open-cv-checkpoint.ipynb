{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mostly OCR engines have assumptions for 300 dpi images (but mobile phones clicked images are not that dpi and we would be working on primarily such kind of images only)\n",
    "\n",
    "Havingdeskewed, grayscaled and binarized the input images, we mainly focus onperforming Multi-Frame Superresolution to produce a high resolutionimage - from a series of low resolution input images - which can then be fedas input to Tesseract OCR.\n",
    "\n",
    "Three main algorithm to be used for the same:-\n",
    "    1. Superresolution- Optical Flow - best performance results but computitionally slow\n",
    "    2. Farsiu’s paper (with L1 NormMinimization) - Fails for skewed or lateral shifts of more than few pixels in any direction.\n",
    "    3. Blind Deconvolution\n",
    "    \n",
    "\n",
    "### What is OCR (Optical Character Recognition)\n",
    "Optical Character Recognition (OCR) is the mechanical or electronicconversion of scanned, printed text images or handwritten text into editable,machine-encoded text for further processing and analysis.\n",
    "\n",
    "###### Appplications\n",
    "    1. Licenseplate recognition systems in various countries at toll stations, roads &CCTV's, \n",
    "    2. image text extraction from natural scene images, scanners and \n",
    "    3. extracting text from scanned documents, cards, printers.\n",
    "    \n",
    "###### Problems/Challenges with default Tesseract Engine: (Open Source OCR product built in Java and can be forked - started by HP and later google took over and released on  )\n",
    "\n",
    "    1. Fails to distinguish between certain characters:\n",
    "        a. Pretty difficult to distinguish between ‘0’(Zero) and an ‘o’(oh) on the machine.\n",
    "        b.\n",
    "        \n",
    "    2. Fails for certain Background/foreground colour combinations\n",
    "        a. \n",
    "        b.\n",
    "        \n",
    "    3. Number of columns of text on a page,or even the size of the text affect the efficiency of the OCR\n",
    "    \n",
    "    4. If the text in an image is beyond a certain height - roughly 36 points or so, the OCR considers it to be graphics and doesn't give any output.\n",
    "    \n",
    "    5. Pre-processing steps using in tesseract by default is not sufficient, we need to process the image before passing it to tesseract.\n",
    "    \n",
    "    6. Although Tesseract has been modified to deal with UTF-8 characters, itmay not work well with languages that possess complex characters, orconnected scripts such as Arabic.\n",
    "    \n",
    "    7. \n",
    "    \n",
    "    \n",
    "###### Tesseract Architecture:\n",
    "    1. Organised in layered and structured format:-\n",
    "    \n",
    "        1. Adaptive Thresholding - converting image into binary images by setting up a certain threshold (all pixels > threshold would be kept others would be set to 0)\n",
    "        \n",
    "        2. Connected Component Analysis - To extract character outlines (useful because it uses white text with black background)\n",
    "        \n",
    "            1. Outlines are analysed and stored (gathered together as blobs)\n",
    "            2. Blobs organised as text lines, which are broken into words.\n",
    "            3. 1st pass of recognition process attempts to recognize each work in turn. Satisfactory words are passed to an adaptive classifier, which needs to be trained beforehand appropriately.\n",
    "            4. The more we train the classifier, the better experience it has and the better it will perform for other words.\n",
    "            5. Lessons learned by adaptive trainer is employed in a second pass, which attempts to recognize the words that were not recognized satisfactorily in the 1st pass.\n",
    "            6. Fuzzy spaces resolved and text checked for the small caps.\n",
    "            7. Digital, editable text is outputted.\n",
    "            \n",
    "        \n",
    "        \n",
    "![TesseractFlow](img/01_TesseractFlow.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tesseract command at the Terminal followed by 2 arguments:\n",
    "    1. image file name that contains text \n",
    "    2. output text file in which, extracted text is stored (no need to provide extension of file as it is by default .txt)\n",
    "    \n",
    "As Tesseract supports various languages, the language training data file mustbe kept in the tessdata folder.\n",
    "\n",
    "###### Can I increase speed of OCR?\n",
    "    If you are running Tesseract 4, you can use the \"fast\" integer models.\n",
    "    Tesseract 4 also uses up to four CPU threads while processing a page, so it will be faster than Tesseract 3 for a single page.\n",
    "    If your computer has only two CPU cores, then running four threads will slow down things significantly and it would be better to use a single thread or maybe a maximum of two threads! Using a single thread eliminates the computation overhead of multithreading and is also the best solution for processing lots of images by running one Tesseract process per CPU core.\n",
    "    Set the maximum number of threads using the environment variable OMP_THREAD_LIMIT.\n",
    "To disable multithreading, use OMP_THREAD_LIMIT=1.\n",
    "\n",
    "\n",
    "###### Pre-processing steps available in Tesseract:\n",
    "\n",
    "\n",
    "Tesseract does various image processing operations internally (using the Leptonica library) before doing the actual OCR. It generally does a very good job of this, but there will inevitably be cases where it isn't good enough, which can result in a significant reduction in accuracy. \n",
    "\n",
    "###### Use tessedit_write_images = true to save the file after default preprocessing steps done by Tesseract.\n",
    "\n",
    "    1. Grayscaling:\n",
    "        1. GrayScaling essentially is converting a coloured body of animage to grayscale( black and white).\n",
    "        2. A grayscaled image can reveal moreinformation than a normal coloured one. \n",
    "        3. Normally a computer can represent upto 256 levels of Gray colour. \n",
    "        4. This process is widely used in a lot of real-time applications such as CCTV’s and traffic light cameras. \n",
    "        5. **Why not sufficient inbuilt grayscaling is sufficient**:\n",
    "            Not sufficient due to low resolution of input, further pre-processing and DPI Enhancement required\n",
    "            \n",
    "    2. Binarization:\n",
    "        1. This works by choosing a threshold value and classifying all pixels above this and below this value.\n",
    "        2. Then we just normalize the images to that threshold value. \n",
    "        3. The main reason why adaptive image binarization is commonly used is it’s ability to adapt the threshold value depending on the image color levels, thereby removing the requirement for manual setting of the threshold value as it automatically“adapts” to the input image.\n",
    "        4. **Why not sufficient inbuilt Binarization is sufficient**:\n",
    "            This might fail, notably in cases of black background with white foreground in the input image.\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "###### Process of capturing an image or video from real-life scenes requires following steps:\n",
    "\n",
    "    1. Sampling: Transformation of input scene (regarded as a continuous intensity distribution) to a discrete system without aliasing.\n",
    "    \n",
    "    2. Geometric Transformation: Transformation such as Translation or Rotation, to remove noise due to camera or lens position\n",
    "    \n",
    "    3. Blur: Motion scenes or issue with lens system\n",
    "    \n",
    "    4. Subsampling: With this, sensor only integrates the number of pixels at its disposal (photosites).\n",
    "\n",
    "![Process of capturing images](img/2_ProcessOfCapturingImages.JPG)\n",
    "\n",
    "            \n",
    "            \n",
    "###### Pre-processing steps (customized):\n",
    "\n",
    "    1. Luminous Gray Scaling:\n",
    "        1. This algorithm says that human eye is not uniform across different colors.\n",
    "        2. Human perceive green more strongly than red and red more strongly than blue. \n",
    "                (more sensitive to green - biological evolution) -\n",
    "                        (natural world have more shades of green color - oceans are not so visible to humans)\n",
    "                        [Human vision system ismost sensitive to green and greenish-yellow colours (the colour of Sunlight)]\n",
    "        3. Instead of treating RGB colors equally, a good grayscale conversion below based on how the human eye perceives it:\n",
    "                                Gray = (Red * 0.2126 + Green * 0.7152 + Blue * 0.0722)\n",
    "                                \n",
    "    2. Deskewing:\n",
    "        1. Removes the existing skew in the image, which is the angle of the text with respect to the horizontal axis. \n",
    "        2. This is done using the HoughTransform.\n",
    "        3. Hough transform is a feature extraction technique used in computer vision, and digital image processing. \n",
    "        4. ClassicalHough Transform identifies lines of text in the image. \n",
    "        5. The rotation angle is first determined using the Hough Transform and then the text box (after having estimated a border around the body of text) is rotated by that angle primarily using certain in-built OpenCV functions that does the simple affineTransform of rotation.\n",
    "        \n",
    "        ![Deskewing](img/4_deskewing.jpg)\n",
    "                \n",
    "    3. Linearization: (Methods ??)\n",
    "        1. Linearization is used to adjust the image from blurs and remove fuzzy edges.\n",
    "        2. Fuzzy edges are caused due to motion while capturing the images.\n",
    "        3. Linearization technique handles every fuzzy edge one by one\n",
    "        \n",
    "    4. Pixelation: (Methods ?? Advantages ??)\n",
    "        1. It is used for character pixelation. \n",
    "        2. Pixelation is defined as displaying the individual pixels of the digitized images. \n",
    "        3. In this we display block for each pixel at a distance to each other which is apparent to the users.\n",
    "        \n",
    "    5. Rescaling:\n",
    "        1. Tesseract works best on images which have a DPI of at least 300 dpi, so it may be beneficial to resize images.\n",
    "        \n",
    "    6. Binarization:\n",
    "        This is converting an image to black and white. Tesseract does this internally, but the result can be suboptimal, particularly if the page background is of uneven darkness.\n",
    "        \n",
    "    7. Noise Removal:\n",
    "        Noise is random variation of brightness or colour in an image, that can make the text of the image more difficult to read. Certain types of noise cannot be removed by Tesseract in the binarisation step, which can cause accuracy rates to drop.\n",
    "        \n",
    "    8. Border Removal:\n",
    "        Scanned pages often have dark borders around them. These can be erroneously picked up as extra characters, especially if they vary in shape and gradation.\n",
    "        \n",
    "    5. Super-Resolution:\n",
    "        1. Designed to increase the spatical resolution using multiple low resolution images of the same object/text.\n",
    "        2. OLD METHODS - to convert low resolution images to high resolution images\n",
    "            1. Interpolation (Bilinear, Bicubic) - by changing the spatial frequency of amplitute spectrum of image.\n",
    "            2. Image Sharpening\n",
    "        3. However, SR method aims at estimating the missing high-resolution details.\n",
    "\n",
    "\n",
    "###### Super-Resolution (Types):\n",
    "    1. Multi-Frame SR:\n",
    "        1. As name suggests, it relies on the presense of multiple frames, mutually misaligned and possibly originated by different geometric transformations, related to the same scene.\n",
    "        2. These multiple frames are conveniently fused together to form a single High-Resolution (HR) output image.\n",
    "        3. As a result, the formed image will contain an amount of detail that is not strictly present in any of the single input images i.e. new information will be created.\n",
    "        \n",
    "    2. Single-Frame SR:\n",
    "        1. Poses a greater difficulty though as we try to estimate the frequency detail of a High-resolution image from as low as 1 input image.\n",
    "        \n",
    "        \n",
    "###### How to represent problem of Super-Resolution?\n",
    "\n",
    "There are many factors which constrain the achievable resolution of any device.\n",
    "\n",
    "The input image scene can be represented as an intensity distribution I(x,y) which is seen to be warped at the camera lens due to the relative motion between the scene and the camera. \n",
    "\n",
    "The images are blurred due atmospheric Turbulence (Hatm(x,y)) and also due to motion between the camera lens and the scene(Motion Blur)(Hcam(x,y)). \n",
    "\n",
    "Then they arediscretized (as converted to digital) at the CCD(Charge Coupling Device) toyield a noisy frame Y[m,n](due to extraneous as well as camera factors).\n",
    "\n",
    "                                Y[m,n]=[Hcam(x,y)**F(Hatm(x,y)**X(x,y))] | + V(m,n)\n",
    "                                \n",
    "V(m,n) -> System Noise\n",
    "F-> Warping Operator\n",
    "**-> 2-dimensional Convolution Operator\n",
    "| ->Discretizing Operator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Multi-Frame SuperResolution using Optical Flow::\n",
    "    A major difficulty of the super-resolution process is to estimate the warping function to build the super-resolution image.\n",
    "    The algorithm focusses on theBilateral TV-L1 (BTVL1) superresolution method implemented.\n",
    "    The Bilateral TV-L1 uses optical flow toestimate the warping function\n",
    "    Use \"superresolution 1.0\" python module for the same, in case that would help you.\n",
    "    \n",
    "    \n",
    "###### What is optical Flow:\n",
    "    Optical flow is the pattern of apparent motion of image objects betweentwo consecutive frames caused by the movemement of object or camera. Itis a 2D vector field where each vector is a displacement vector showing themovement of points from first frame to second.\n",
    "    \n",
    "![3_OpticalFlowVector.jpg](img/3_OpticalFlowVector.jpg)\n",
    "    \n",
    "This algorithm uses the “farneback” Optical Flow Algorithm-that computesthe optical flow for all points in the frame- with the “Dense Optical Flow”method that detects the salient features and tracks them to correlate forsuccessive video frames. Following this, the superresolution algorithm uses“BTV L1” algorithm that uses the following parameters:\n",
    "    • scale: This is the scale factor\n",
    "    • iterations: This is the iteration count\n",
    "    • tau: This is an asymptotic value of the steepest descent method\n",
    "    • lambda: This is the weight parameter to balance the data term andsmoothness term\n",
    "    • alpha: This is a parameter of spatial distribution in Bilateral-TV\n",
    "    • btvKernelSize: This is the kernel size of the Bilateral-TV filter\n",
    "    • blurKernelSize: This is the Gaussian blur kernel size\n",
    "    • blurSigma: This is the Gaussian blur sigma\n",
    "    • temporalAreaRadius: This is the radius of the temporal search area\n",
    "    • opticalFlow: This is a dense optical-flow algorithm.\n",
    "\n",
    "For each frame, the superresolution algorithm is applied with scale andnumber of iterations as variable; hence the algorithm is computationally veryslow. We display the time taken for each iteration to show so. Even though itgives considerably good results, it is very slow and even might take upto 55minutes for computing the HR image from 9 LR images of size (512*342)\n",
    "\n",
    "\n",
    "\n",
    "###### Multi- Frame Superresolution Using Farsiu’s[16] Approach\n",
    "Due to the excessively slow computational efficiency of the optical flowbased Superresolution algorithm, we next try out the approach mentioned in[16]. This paper proposes to do a L1 Norm Minimization with a robustregularization (BTV- Bilateral Total Variation) to deal with different noiseand data models. It claims to be computationally cheap, works with all sortsof input images and results in images with sharp edges. An L1 NormMinimization is done so as to be able to account also for outliers with abreakdown value of 0.5.\n",
    "\n",
    "                Y [m, n]=[Hcam(x, y) ∗ ∗F(Hatm(x, y) ∗ ∗X(x, y))] ↓ +V [m, n]\n",
    "\n",
    "Where ,\n",
    "    Y[m,n] is digitized noisy frame\n",
    "    Hcam(x,y) is the camera blur\n",
    "    Hatm(x,y) is the atmospheric blur\n",
    "    F is the warping operator\n",
    "    ↓ is the discretizing operator\n",
    "    V [m, n] is the system noise\n",
    "    X(x,y) is the high resolution frame\n",
    "\n",
    "We first look at the code from [18]. However, we realise that it fails for allcases where the shift is greater than 1 pixel in any direction (skew included).The code basically develops its own noise model, generated randomly, forthe input images and applies them to the input frames and estimates theapproximate difference -over a Kernel size ( for all practical purposes, 7 andabove)- to do superresolution. The Regularization done is BTV(BilateralTotal Variation).Then, superresolution takes over by estimating differencebetween the consecutive regularized frames pitted against the noise model,over a certain number of iterations. However, since it fails even for framesextracted from a video taken with a steady hand, we tried to improve it bychanging the DHF (Downsampling and Motion Blur )matrix, which is ourestimated noise model\n",
    "\n",
    "We simply generate a random shift in the ‘x’ and ‘y’ directions along with arandom direction (theta). We then apply a 3*3 affine transform matrix withthe above generated values and perform a Kronecker product with an identity matrix to generate the above DHF. This works as we are notchanging the shifts in the horizontal and vertical directions for each pixel asbefore but for the picture as a whole. Furthermore, we are taking intoaccount the skew- any direction vector can be broken down into itsconstituent x and y direction vectors. We do get considerably better resultswith this. However, it still fails for larger shifts and that primarily, wesuspect, is due to how the algorithm has been implemented. Hence, we moveon to implement superresolution using deconvolution- our topic for the nextsection.\n",
    "\n",
    "\n",
    "###### Geometric layout analysis: GLA\n",
    "    A reading system requires segmentation of text zones from non-textual ones and arrangement in their correct reading order.\n",
    "    Detection & labeling of different zones(blocks) as text body, illustrations, math symbols, & tables in a document is GLA\n",
    "    \n",
    "###### Logical layout analysis: LLA\n",
    "    Text zones play different logical roles inside the document (titles, captions, footnotes, etc.).\n",
    "    This kind of semantic labeling is the scope of the logical layout analysis\n",
    "\n",
    "###### Document layout analysis: DLA = GLA union LLA - typically done before passing image to OCR\n",
    "    The process of identifying and categorizing the regions of interest in the scanned image of a text document\n",
    "    \n",
    "######     Two Approaches:\n",
    "          1. Bottom-up approaches:\n",
    "             1. Iteratively parse a document based on the raw pixel data\n",
    "             2. First parse a document into connected regions of black and white, \n",
    "             3. Then these regions are grouped into words, then into text lines, and finally into text blocks\n",
    "             \n",
    "          2. Top-down approaches:\n",
    "             1. Iteratively cut up a document into columns and blocks based on white space and geometric information\n",
    "             2.  \n",
    "             3. \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN-CV\n",
    "\n",
    "A container Mat is used to store an image in OpenCV.The class Mat represents an n-dimensional dense numerical single-channelor multi-channel array. It can be used to store real or complex-valued vectorsand matrices, grayscale or color images, voxel volumes, vector fields, pointclouds, tensors, histograms (though, very high-dimensional histograms maybe better stored in a SparseMat ). \n",
    "\n",
    " Mat is basically a class with two data parts:the matrix header (containing information such as the size of the matrix, themethod used for storing, at which address is the matrix stored, and so on)and a pointer to the matrix containing the pixel values (taking anydimensionality depending on the method chosen for storing) . The matrixheader size is constant, however the size of the matrix itself may vary fromimage to image and usually is larger by orders of magnitude\n",
    " \n",
    "#Split out each channel\n",
    "blue, green, red = cv2.split(img)\n",
    "\n",
    "\n",
    "# Run canny edge detection on each channel\n",
    "blue_edges = cv2.Canny(blue, 200, 250)\n",
    "green_edges = cv2.Canny(green, 200, 250)\n",
    "red_edges = cv2.Canny(red, 200, 250)\n",
    "\n",
    "# Join edges back into image\n",
    "edges = blue_edges | green_edges | red_edges\n",
    "\n",
    "\n",
    "# Find the contours\n",
    "contours, hierarchy = cv2.findContours(edges.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "# For each contour, find the bounding rectangle and decide\n",
    "# if it's one we care about\n",
    "for index_, contour_ in enumerate(contours):\n",
    "    if DEBUG:\n",
    "        print \"Processing #%d\" % index_\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(contour_)\n",
    "\n",
    "    # Check the contour and it's bounding box\n",
    "    if keep(contour_) and include_box(index_, hierarchy, contour_):\n",
    "        # It's a winner!\n",
    "        keepers.append([contour_, [x, y, w, h]])\n",
    "        if DEBUG:\n",
    "            cv2.rectangle(processed, (x, y), (x + w, y + h), (100, 100, 100), 1)\n",
    "            cv2.putText(processed, str(index_), (x, y - 5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255))\n",
    "    else:\n",
    "        if DEBUG:\n",
    "            cv2.rectangle(rejected, (x, y), (x + w, y + h), (100, 100, 100), 1)\n",
    "            cv2.putText(rejected, str(index_), (x, y - 5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255))\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
