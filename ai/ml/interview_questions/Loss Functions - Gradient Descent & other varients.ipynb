{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss:\n",
    "\n",
    "1. Hyperparameters are the configuration settings used to tune how the model is trained.\n",
    "2. Derivative of (y - y')2 with respect to the weights and biases tells us how loss changes for a given example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goldilocks principle\n",
    "\n",
    "The Goldilocks principle is named by analogy to the children's story The Three Bears, in which a little girl named Goldilocks tastes three different bowls of porridge, and she finds that she prefers porridge which is neither too hot nor too cold, but has just the right temperature.[1] Since the children's story is well known across cultures, the concept of \"just the right amount\" is easily understood and is easily applied to a wide range of disciplines, including developmental psychology, biology,[2] astronomy, economics and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "\n",
    "    Weight Initialization:\n",
    "    \n",
    "        For convex problems, weights can start anywhere (say, all 0s)\n",
    "            Convex: think of a bowl shape\n",
    "            Just one minimum\n",
    "            \n",
    "        Foreshadowing: not true for **neural nets**\n",
    "            Non-convex: think of an egg crate\n",
    "            More than one minimum\n",
    "            Strong dependency on initial values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD & Mini-Batch Gradient Descent\n",
    "\n",
    "    1. Could compute gradient over entire data set on each step, but this turns out to be unnecessary\n",
    "    \n",
    "    2. Computing gradient on small data samples works well\n",
    "        1. On every step, get a new random sample\n",
    "        \n",
    "    3. Varients of Gradient Descent:\n",
    "        1. Stochastic Gradient Descent: one example at a time\n",
    "        2. Mini-Batch Gradient Descent: batches of 10-1000\n",
    "        \n",
    "    4. Loss & gradients are averaged over the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent, a batch is the total number of examples you use to calculate the gradient in a single iteration.\n",
    "\n",
    "\n",
    "So far, we've assumed that the batch has been the entire data set. When working at Google scale, data sets often contain billions or even hundreds of billions of examples. Furthermore, Google data sets often contain huge numbers of features. Consequently, a batch can be enormous. A very large batch may cause even a single iteration to take a very long time to compute.\n",
    "\n",
    "A large data set with randomly sampled examples probably contains redundant data. In fact, redundancy becomes more likely as the batch size grows. Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches.\n",
    "\n",
    "What if we could get the right gradient on average for much less computation? By choosing examples at random from our data set, we could estimate (albeit, noisily) a big average from a much smaller one. Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "\n",
    "\n",
    "Mini-batch stochastic gradient descent (mini-batch SGD) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
