{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install nltk\n",
    "# !pip install plotly\n",
    "# !pip install cufflinks\n",
    "# !python -m spacy.en.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/home/jupyter/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f7cf44040934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "import cufflinks as cf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "cf.go_offline()\n",
    "offline.init_notebook_mode()\n",
    "PLOT_API_KEY = os.environ['SECRET_ENV_AARON_PLOT_API_KEY']\n",
    "PLOTLY_USERNAME = os.environ['SECRET_ENV_AARON_PLOTLY_USERNAME']\n",
    "py.sign_in(PLOTLY_USERNAME,PLOT_API_KEY)\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "def rep_sentences(texts):\n",
    "    html = []\n",
    "    for text in texts:\n",
    "        html.append(rep_sentence(text))\n",
    "    return HTML(\"\".join(html))\n",
    "\n",
    "def rep_sentence(text, display_pos = True):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'purple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    doc = nlp(unicode(text))\n",
    "    n_words = len(doc)\n",
    "    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    html.append(\"<tr>\")            \n",
    "    for i in xrange(n_words):\n",
    "        word_string= doc[i].orth_\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "    html.append(\"</tr>\")\n",
    "    if display_pos:\n",
    "        html.append(\"<tr>\")            \n",
    "        for i in xrange(n_words):\n",
    "            pos = doc[i].pos_\n",
    "            color = pos_to_color[pos]\n",
    "            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n",
    "        html.append(\"</tr>\")\n",
    "    html = \"\".join(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "def custom_tag_table(list_of_word_tag_tuples):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'MediumPurple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    \n",
    "    n_words = len(list_of_word_tag_tuples)\n",
    "    words, pos_list = zip(*list_of_word_tag_tuples)\n",
    "    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    for i in xrange(n_words):\n",
    "        html.append(\"<tr>\")            \n",
    "        word_string= words[i]\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "        row = []\n",
    "        pos_sublist = pos_list[i]\n",
    "        for pos in pos_sublist:\n",
    "            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n",
    "            #print entry\n",
    "            row.append(entry)\n",
    "        row = \"\".join(row)\n",
    "        html.append(\"<td>{}</td>\".format(row))\n",
    "        html.append(\"</tr>\")\n",
    "    return \"\".join(html)\n",
    "        \n",
    "    \n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "#read nltk corpora\n",
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = nltk_corpus(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    \n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter\n",
    "\n",
    "universal_tags = [\n",
    "     ['Open Class Words','ADJ','adjective']\n",
    "    ,['Open Class Words','ADV','adverb']\n",
    "    ,['Open Class Words','INTJ','interjection']\n",
    "    ,['Open Class Words','NOUN','noun']\n",
    "    ,['Open Class Words','PROPN','proper noun']\n",
    "    ,['Open Class Words','VERB','verb']\n",
    "    ,['Closed Class Words','ADP','adposition']\n",
    "    ,['Closed Class Words','AUX','auxiliary']\n",
    "    ,['Closed Class Words','CCONJ','coordination conjunction']\n",
    "    ,['Closed Class Words','DET','determiner']\n",
    "    ,['Closed Class Words','NUM','numeral']\n",
    "    ,['Closed Class Words','PART','particle']\n",
    "    ,['Closed Class Words','PRON','pronoun']\n",
    "    ,['Closed Class Words','SCONJ','subordinating conjection']\n",
    "    ,['Other','PUNCT','punctuation']\n",
    "    ,['Other','SYM','symbol']\n",
    "    ,['Other','X','other']\n",
    "]\n",
    "tag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech'])\n",
    "tag_table = tag_table.set_index(['Category','Abbrev'])\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nlp = spacy.load('en', parser = False,entity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### POS Tagsets\n\n##### Universal: token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "tag_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "##### Penn: token.tag_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Part of Speech Tags for Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.ADP{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.DET{background-color:yellow;}</style><style>.PRON{background-color:orange;}</style><style>.VERB{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.PRON{background-color:SkyBlue;}</style><style>.VERB{background-color:red;}</style><style>.NOUN{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'I get a discount on newspapers.'\n",
    "sentence2 = 'I discount that newspaper.'\n",
    "\n",
    "rep_sentences([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"65d04935-b01f-48bf-81ab-037df879d925\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"65d04935-b01f-48bf-81ab-037df879d925\", [{\"name\": \"None\", \"text\": \"\", \"y\": [0.5501011898853854, 0.3026099083655352, 0.08579691030185371, 0.05082373445351574, 0.009092790199637634, 0.0006863752824148297, 0.0008890915116574449], \"marker\": {\"color\": \"rgba(255, 153, 51, 0.6)\", \"line\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"width\": 1}}, \"x\": [1, 2, 3, 4, 5, 6, 7], \"type\": \"bar\", \"orientation\": \"v\"}], {\"title\": \"45% of Brown Corpus of Spacy-tagged tokens are ambiguous\", \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"xaxis1\": {\"tickfont\": {\"color\": \"#4D5663\"}, \"title\": \"Unique Parts of Speech in Brown Corpus\", \"showgrid\": true, \"zerolinecolor\": \"#E1E5ED\", \"gridcolor\": \"#E1E5ED\", \"titlefont\": {\"color\": \"#4D5663\"}}, \"yaxis1\": {\"tickfont\": {\"color\": \"#4D5663\"}, \"title\": \"Percent of Unique Words in Vocabulary\", \"showgrid\": true, \"zerolinecolor\": \"#E1E5ED\", \"gridcolor\": \"#E1E5ED\", \"titlefont\": {\"color\": \"#4D5663\"}}, \"titlefont\": {\"color\": \"#4D5663\"}, \"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #this example uses shows the frequency of tokens with multiple\n",
    "# #parts of speech\n",
    "\n",
    "\n",
    "# #create a dictionary of sets\n",
    "# unique_tag_dictionary = defaultdict(set)\n",
    "\n",
    "# for doc in nlp.pipe(nltk_reader('brown'), n_threads=4):\n",
    "#     for token in doc:\n",
    "#         #add the token's part of speech to the lexeme's set\n",
    "#         unique_tag_dictionary[token.orth_].add(token.pos_)\n",
    "        \n",
    "# #for each word, how many unique POS's were found?\n",
    "# n_word_senses = {word: len(senses) for word, senses in unique_tag_dictionary.items()}\n",
    "\n",
    "# #map each word to its number of senses, and count\n",
    "# ambiguous_word_counts = Counter()\n",
    "# for doc in nlp.pipe(nltk_reader('brown'), n_threads=4):\n",
    "#     word_senses = map(lambda token: n_word_senses[token.orth_] , doc)\n",
    "#     ambiguous_word_counts.update(word_senses)\n",
    "\n",
    "# #normalize and feed to pandas    \n",
    "# N = float(sum(ambiguous_word_counts.values()))\n",
    "# plot_data = pd.Series(ambiguous_word_counts).map(lambda x: x / N)\n",
    "\n",
    "#plot\n",
    "plot_data.iplot(kind='bar'\n",
    "                , title = \"45% of Brown Corpus of Spacy-tagged tokens are ambiguous\"\n",
    "                , xTitle = \"Unique Parts of Speech in Brown Corpus\"\n",
    "                , yTitle= \"Percent of Unique Words in Vocabulary\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "#pick first sentence\n",
    "sent = list(doc.sents)[0]\n",
    "\n",
    "#make a list of (word, unique_tags) pairs\n",
    "word_tag_list = [(token.orth_, list(unique_tag_dictionary[token.lemma_])) for token in sent]\n",
    "\n",
    "#display\n",
    "HTML(custom_tag_table(word_tag_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Inferring Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.ADP{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.PROPN{background-color:yellow;}</style><style>.DET{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><style>.PART{background-color:brown;}</style><style>.ADJ{background-color:purple;}</style><style>.VERB{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = u'I was loble to find the effix by klepping the Dongle search engine.'\n",
    "document = nlp(text)\n",
    "HTML(rep_sentence(document, display_pos = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.ADP{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.PROPN{background-color:yellow;}</style><style>.DET{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><style>.PART{background-color:brown;}</style><style>.ADJ{background-color:purple;}</style><style>.VERB{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADJ'>ADJ</span></td><td><span class='PART'>PART</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='PROPN'>PROPN</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(rep_sentence(document, display_pos = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Determinants of Part of Speech:\n* Word: some words can only be used in a single way; we can memorize these.\n* Word shape: if the first letter is capitalized, its likely a proper noun.\n* Neighboring part of speech: there are common patterns, such as noun phrases commonly following a determiner. to the beach\n\n\n\n| Feature | Notes | Example|\n|------|------|------|\n|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n|?|?|?|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "corpus = nltk_corpus('brown')\n",
    "all_data = np.array(corpus.tagged_sents(tagset='brown'))\n",
    "all_data = [zip(*i) for i in all_data]\n",
    "train, test = train_test_split(all_data, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "#EXPERIMENTAL\n",
    "from spacy.symbols import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from nltk.tag import tagset_mapping\n",
    "from spacy.en import English\n",
    "from spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n",
    "                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n",
    "                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n",
    "                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n",
    "                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n",
    "\n",
    "def validate(test_data, tagger):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tagmap = tagger.vocab.morphology.tag_map\n",
    "    for words, tags in test_data:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        tagger(doc)\n",
    "        predictions = map(lambda token: tagmap[token.tag_], doc)\n",
    "        actual = map(lambda tag: tagmap[tag], tags)\n",
    "        \n",
    "        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n",
    "        n_correct = len(correct_predictions)\n",
    "        correct += n_correct\n",
    "        total += len(words)\n",
    "        \n",
    "    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n",
    "    return result\n",
    "\n",
    "def generate_tagmap():\n",
    "    def adjust_value(x):\n",
    "        if x == '.':\n",
    "            val = PUNCT\n",
    "        elif x=='PRT':\n",
    "            val = PART\n",
    "        else:\n",
    "            val = getattr(spacy.symbols, x)\n",
    "        return {POS:val}\n",
    "    nltk_map = tagset_mapping('en-brown','universal')\n",
    "    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n",
    "    return adj_map\n",
    "\n",
    "features = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n",
    ",(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n",
    ",(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n",
    ",(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n",
    ",(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n",
    "\n",
    "tagmap = generate_tagmap()\n",
    "gold_tagmap = nlp.vocab.morphology.tag_map\n",
    "for key in tagmap:\n",
    "    gold_tagmap[key] = tagmap[key]\n",
    "\n",
    "vocab = nlp.vocab\n",
    "tagger = nlp.tagger\n",
    "\n",
    "\n",
    "pretraining_accuracy = validate(test, tagger)\n",
    "print(pretraining_accuracy)\n",
    "\n",
    "for i in range(10):\n",
    "    train_fold, test_fold = train_test_split(train, test_size = .2)\n",
    "    for words, tags in train_fold:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        tagger.update(doc, gold)\n",
    "    current_accuracy = validate(test_fold, tagger)\n",
    "    print(current_accuracy)\n",
    "    np.random.shuffle(train)\n",
    "tagger.model.end_training()\n",
    "\n",
    "posttraining_accuracy = validate(test, tagger)\n",
    "print(posttraining_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from nltk.tag import tagset_mapping\n",
    "from spacy.language import Language\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n",
    "                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n",
    "                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n",
    "                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n",
    "                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n",
    "\n",
    "def validate(test_data, tagger):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tagmap = tagger.vocab.morphology.tag_map\n",
    "    for words, tags in test_data:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        tagger(doc)\n",
    "        predictions = map(lambda token: tagmap[token.tag_], doc)\n",
    "        actual = map(lambda tag: tagmap[tag], tags)\n",
    "        \n",
    "        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n",
    "        n_correct = len(correct_predictions)\n",
    "        correct += n_correct\n",
    "        total += len(words)\n",
    "        \n",
    "    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n",
    "    return result\n",
    "\n",
    "def generate_tagmap():\n",
    "    def adjust_value(x):\n",
    "        if x == '.':\n",
    "            val = PUNCT\n",
    "        elif x=='PRT':\n",
    "            val = PART\n",
    "        else:\n",
    "            val = getattr(spacy.symbols, x)\n",
    "        return {POS:val}\n",
    "    nltk_map = tagset_mapping('en-brown','universal')\n",
    "    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n",
    "    return adj_map\n",
    "\n",
    "N_ITER = 1\n",
    "\n",
    "\n",
    "def make_tagger(vocab, templates):\n",
    "    model = spacy.tagger.TaggerModel(templates)\n",
    "    return spacy.tagger.Tagger(vocab,model)\n",
    "\n",
    "\n",
    "\n",
    "tagmap = generate_tagmap()\n",
    "vocab = Vocab(tag_map = tagmap)\n",
    "tagger = make_tagger(vocab, nlp.tagger.feature_templates)\n",
    "\n",
    "\n",
    "print(\"Itn.\\ttrain acc %\\tdev acc %\")\n",
    "for itn in xrange(N_ITER):\n",
    "    print(\"Iteration {}\".format(itn)) \n",
    "    scorer = Scorer()\n",
    "    COUNTER = 0\n",
    "    for words, tags in train[:100]:\n",
    "        words = map(unicode, words)\n",
    "        doc = Doc(vocab, words = map(unicode, words))\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        tagger.update(doc, gold)\n",
    "        scorer.score(doc, gold)\n",
    "        COUNTER += 1\n",
    "        if COUNTER % 1000 == 0:\n",
    "            print(COUNTER, correct/total)    \n",
    "        \n",
    "    train_acc = correct/total\n",
    "    np.random.shuffle(train)\n",
    "    print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "print(\"Itn.\\ttrain acc %\\tdev acc %\")\n",
    "for itn in xrange(10):\n",
    "    print(\"Iteration {}\".format(itn)) \n",
    "    scorer = Scorer()\n",
    "    COUNTER = 0\n",
    "    for words, tags in train[:10000]:\n",
    "        words = map(unicode, words)\n",
    "        doc = Doc(vocab, words = map(unicode, words))\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        tagger.update(doc, gold)\n",
    "        scorer.score(doc, gold)\n",
    "    print(scorer.tags_acc)\n",
    "    np.random.shuffle(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "for itn in range(5):\n",
    "    print(\"Iteration {}\".format(itn)) \n",
    "    correct, total = 0., 0.\n",
    "    COUNTER = 0\n",
    "    for words, tags in train[10000:20000]:\n",
    "        words = map(unicode, words)\n",
    "        doc = Doc(vocab, words = map(unicode, words))\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        correct += tagger.update(doc, gold)\n",
    "        total += len(words)\n",
    "        COUNTER += 1\n",
    "        if COUNTER % 1000 == 0:\n",
    "            print(COUNTER, correct/total)    \n",
    "    train_acc = correct/total\n",
    "    np.random.shuffle(train)\n",
    "    print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "tagger.update(doc, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "doc = nlp.tokenizer(\" \".join(map(unicode,words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "TaggerModel.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from nltk.tag import tagset_mapping\n",
    "from spacy.en import English\n",
    "from spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n",
    "                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n",
    "                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n",
    "                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n",
    "                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n",
    "\n",
    "def validate(test_data, tagger):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for words, tags in test_data:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        tagger(doc)\n",
    "        predictions = map(lambda token: tagmap[token.tag_], doc)\n",
    "        actual = map(lambda tag: tagmap[tag], tags)\n",
    "        \n",
    "        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n",
    "        n_correct = len(correct_predictions)\n",
    "        correct += n_correct\n",
    "        total += len(words)\n",
    "        \n",
    "    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n",
    "    return result\n",
    "\n",
    "def generate_tagmap():\n",
    "    def adjust_value(x):\n",
    "        if x == '.':\n",
    "            val = PUNCT\n",
    "        elif x=='PRT':\n",
    "            val = PART\n",
    "        else:\n",
    "            val = getattr(spacy.symbols, x)\n",
    "        return {POS:val}\n",
    "    nltk_map = tagset_mapping('en-brown','universal')\n",
    "    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n",
    "    return adj_map\n",
    "\n",
    "features = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n",
    ",(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n",
    ",(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n",
    ",(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n",
    ",(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n",
    "\n",
    "tagmap = generate_tagmap()\n",
    "vocab = Vocab(tag_map = tagmap)\n",
    "tagger = Tagger(vocab)\n",
    "\n",
    "\n",
    "pretraining_accuracy = validate(test, tagger)\n",
    "print(pretraining_accuracy)\n",
    "\n",
    "for i in range(10):\n",
    "    train_fold, test_fold = train_test_split(train, test_size = .2)\n",
    "    for words, tags in train_fold:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        tagger.update(doc, gold)\n",
    "    current_accuracy = validate(test_fold, tagger)\n",
    "    print(current_accuracy)\n",
    "    np.random.shuffle(train)\n",
    "tagger.model.end_training()\n",
    "\n",
    "posttraining_accuracy = validate(test, tagger)\n",
    "print(posttraining_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from nltk.tag import tagset_mapping\n",
    "from spacy.en import English\n",
    "from spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n",
    "                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n",
    "                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n",
    "                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n",
    "                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n",
    "def generate_tagmap():\n",
    "    def adjust_value(x):\n",
    "        if x == '.':\n",
    "            val = PUNCT\n",
    "        elif x=='PRT':\n",
    "            val = PART\n",
    "        else:\n",
    "            val = getattr(spacy.symbols, x)\n",
    "        return {POS:val}\n",
    "    nltk_map = tagset_mapping('en-brown','universal')\n",
    "    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n",
    "    return adj_map\n",
    "\n",
    "features = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n",
    ",(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n",
    ",(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n",
    ",(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n",
    ",(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n",
    "\n",
    "tagmap = generate_tagmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "nlp.vocab.morphology.tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.syntax.arc_eager import ArcEager\n",
    "from spacy.syntax.ner import BiluoPushDown\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.syntax.parser import Parser\n",
    "from spacy.syntax.nonproj import PseudoProjectivity\n",
    "from os import path\n",
    "import shutil\n",
    "\n",
    "def train(Language, train_loc, model_dir, n_iter=15, feat_set=u'ner', seed=0,\n",
    "      gold_preproc=False, n_sents=0):\n",
    "    print(\"Setup model dir\")\n",
    "    ner_model_dir = path.join(model_dir, 'ner')\n",
    "    if path.exists(ner_model_dir):\n",
    "        shutil.rmtree(ner_model_dir)\n",
    "    os.mkdir(ner_model_dir)\n",
    "    nlp = Language(path=model_dir, tagger=False, parser=False, entity=False)\n",
    "    labels = BiluoPushdown.get_labels(gold_tuples)\n",
    "    Config.write(ner_model_dir, 'config', features=feat_set, seed=seed,\n",
    "             labels=labels)\n",
    "    nlp.entity = Parser.from_dir(ner_model_dir)\n",
    "    nlp.tagger = Tagger.blank(nlp.vocab, Tagger.default_templates())\n",
    "    for itn in range(nr_iter):\n",
    "        for _, sents in gold_tuples:\n",
    "            for annot_tuples, _ in sents:\n",
    "                tokens = nlp.tokenizer.tokens_from_list(annot_tuples[1])\n",
    "                nlp.tagger.tag_from_strings(tokens, annot_tuples[2])\n",
    "                gold = GoldParse(tokens, annot_tuples)\n",
    "                loss += nlp.entity.train(tokens, gold)\n",
    "        random.shuffle(gold_tuples)\n",
    "    nlp.entity.model.end_training()\n",
    "    return nlp\n",
    "\n",
    "\n",
    "train(spacy.language.Language, 'train',str(p.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "str(p.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "map(lambda x: tagmap[x], train_fold[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import Tagger\n",
    "vocab = Vocab(tag_map = tagmap)\n",
    "\n",
    "\n",
    "#doc = Doc(vocab, words = map(unicode,['i','like','green','eggs']))\n",
    "doc = nlp.tokenizer(u\"I like green eggs\")\n",
    "vocab = nlp.vocab\n",
    "tagger1 = Tagger(vocab)\n",
    "tagger1(doc)\n",
    "map(lambda x: (x.pos_, x.orth_), doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "nlp.tagger.feature_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "doc = Doc(vocab, words = map(unicode,['i','like','green','eggs']))\n",
    "tagger(doc)\n",
    "map(lambda x: (x.pos_, x.orth_), doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "d = Doc(vocab, words = test[0][0])\n",
    "tagger(d)\n",
    "map(lambda x: (x.orth_, x.pos_, x.tag_) , d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def validate(test_data, tagger):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for words, tags in test_data:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        tagger(doc)\n",
    "        predictions = map(lambda token: tagmap[token.tag_], doc)\n",
    "        actual = map(lambda tag: tagmap[tag], tags)\n",
    "        \n",
    "        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n",
    "        n_correct = len(correct_predictions)\n",
    "        correct += n_correct\n",
    "        total += len(words)\n",
    "        \n",
    "    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "toks = nlp.tokenizer.tokens_from_list(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "DATA1 = zip(*DATA)\n",
    "sents, tags = DATA1\n",
    "sents = [map(unicode, i) for i in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "DATA[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def map_to_unicode(list_of_strings):\n",
    "    return map(unicode, list_of_strings)\n",
    "def make_docs(doc_tuples):\n",
    "    \n",
    "    return [nlp.tokenizer.tokens_from_list(map_to_unicode(sent_tuples[0]))\n",
    "                    for sent_tuples in doc_tuples]\n",
    "docs = make_docs(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "for doc, tokens_tags_tuple in zip(docs, DATA):\n",
    "    tokens = tokens_tags_tuple[0]\n",
    "    tags = tokens_tags_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "nlp.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "doc = docs[0]\n",
    "toks_tags_tuple = DATA[0]\n",
    "tags = toks_tags_tuple[1]\n",
    "GoldParse.from_annot_tuples(doc, tags)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def make_golds(docs, doc_tuples):\n",
    "    return [GoldParse.from_annot_tuples(doc, tokens_tags_tuple[1])\n",
    "            for doc, tokens_tags_tuple in zip(docs, doc_tuples)]\n",
    "golds = make_golds(docs, DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "d = nlp.tokenizer.tokens_from_list([u'The',\n",
    " u'Fulton',\n",
    " u'County',\n",
    " u'Grand',\n",
    " u'Jury',\n",
    " u'said',\n",
    " u'Friday',\n",
    " u'an',\n",
    " u'investigation',\n",
    " u'of',\n",
    " u\"Atlanta's\",\n",
    " u'recent',\n",
    " u'primary',\n",
    " u'election',\n",
    " u'produced',\n",
    " u'``',\n",
    " u'no',\n",
    " u'evidence',\n",
    " u\"''\",\n",
    " u'that',\n",
    " u'any',\n",
    " u'irregularities',\n",
    " u'took',\n",
    " u'place',\n",
    " u'.'])\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "GoldParse.from_annot_tuples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.gold import GoldParse\n",
    "def make_golds(docs, paragraph_tuples):\n",
    "        if len(docs) == 1:\n",
    "            return [GoldParse.from_annot_tuples(docs[0], sent_tuples[0])\n",
    "                    for sent_tuples in paragraph_tuples]\n",
    "        else:\n",
    "            return [GoldParse.from_annot_tuples(doc, sent_tuples[0])\n",
    "                    for doc, sent_tuples in zip(docs, paragraph_tuples)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "def score_model(scorer, nlp, raw_text, annot_tuples):\n",
    "    if raw_text is None:\n",
    "        tokens = nlp.tokenizer.tokens_from_list(annot_tuples[1])\n",
    "    else:\n",
    "        tokens = nlp.tokenizer(raw_text)\n",
    "    nlp.tagger(tokens)\n",
    "    gold = GoldParse(tokens, annot_tuples)\n",
    "    scorer.score(tokens, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'this is a test')\n",
    "nlp.tagger.feature_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "doc = Doc(vocab, words = [u'I',u'like',u'green',u'eggs',u'and',u'ham'])\n",
    "#doc = Doc(vocab, words = DATA[5][0])\n",
    "tagger(doc)\n",
    "map(lambda x: (x.orth_, x.pos_, x.tag_), doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "DATA[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "tagger.vocab.morphology.tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from thinc.linear import avgtron\n",
    "from __future__ import unicode_literals\n",
    "from spacy.symbols import *\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "\n",
    "TAG_MAP = {\n",
    "    \".\":        {POS: PUNCT, \"PunctType\": \"peri\"},\n",
    "    \",\":        {POS: PUNCT, \"PunctType\": \"comm\"},\n",
    "    \"-LRB-\":    {POS: PUNCT, \"PunctType\": \"brck\", \"PunctSide\": \"ini\"},\n",
    "    \"-RRB-\":    {POS: PUNCT, \"PunctType\": \"brck\", \"PunctSide\": \"fin\"},\n",
    "    \"``\":       {POS: PUNCT, \"PunctType\": \"quot\", \"PunctSide\": \"ini\"},\n",
    "    \"\\\"\\\"\":     {POS: PUNCT, \"PunctType\": \"quot\", \"PunctSide\": \"fin\"},\n",
    "    \"''\":       {POS: PUNCT, \"PunctType\": \"quot\", \"PunctSide\": \"fin\"},\n",
    "    \":\":        {POS: PUNCT},\n",
    "    \"(\":        {POS: PUNCT},\n",
    "    \")\":        {POS: PUNCT},\n",
    "    \"$\":        {POS: SYM, \"Other\": {\"SymType\": \"currency\"}},\n",
    "    \"#\":        {POS: SYM, \"Other\": {\"SymType\": \"numbersign\"}},\n",
    "    'ADJ':      {POS: ADJ},\n",
    "    'ADP':      {POS: ADP},\n",
    "    'ADV':      {POS: ADV},\n",
    "    \"AFX\":      {POS: ADJ,  \"Hyph\": \"yes\"},\n",
    "    'CONJ':      {POS: CONJ},\n",
    "    \"CC\":       {POS: CONJ, \"ConjType\": \"coor\"},\n",
    "    \"CD\":       {POS: NUM, \"NumType\": \"card\"},\n",
    "    \"DT\":       {POS: DET},\n",
    "    \"DET\":       {POS: DET},\n",
    "    \"EX\":       {POS: ADV, \"AdvType\": \"ex\"},\n",
    "    \"FW\":       {POS: X, \"Foreign\": \"yes\"},\n",
    "    \"HYPH\":     {POS: PUNCT, \"PunctType\": \"dash\"},\n",
    "    \"IN\":       {POS: ADP},\n",
    "    \"JJ\":       {POS: ADJ, \"Degree\": \"pos\"},\n",
    "    \"JJR\":      {POS: ADJ, \"Degree\": \"comp\"},\n",
    "    \"JJS\":      {POS: ADJ, \"Degree\": \"sup\"},\n",
    "    \"LS\":       {POS: PUNCT, \"NumType\": \"ord\"},\n",
    "    \"MD\":       {POS: VERB, \"VerbType\": \"mod\"},\n",
    "    \"NIL\":      {POS: \"\"},\n",
    "    \"NN\":       {POS: NOUN, \"Number\": \"sing\"},\n",
    "    \"NOUN\":     {POS: NOUN},\n",
    "    \"NNP\":      {POS: PROPN, \"NounType\": \"prop\", \"Number\": \"sing\"},\n",
    "    \"NNPS\":     {POS: PROPN, \"NounType\": \"prop\", \"Number\": \"plur\"},\n",
    "    \"NNS\":      {POS: NOUN, \"Number\": \"plur\"},\n",
    "    \"NUM\":      {POS: NUM},\n",
    "    \"PDT\":      {POS: ADJ, \"AdjType\": \"pdt\", \"PronType\": \"prn\"},\n",
    "    \"POS\":      {POS: PART, \"Poss\": \"yes\"},\n",
    "    \"PRON\":     {POS: PRON},\n",
    "    \"PRP\":      {POS: PRON, \"PronType\": \"prs\"},\n",
    "    \"PRP$\":     {POS: ADJ, \"PronType\": \"prs\", \"Poss\": \"yes\"},\n",
    "    'PRT':      {POS: PART},\n",
    "    \"RB\":       {POS: ADV, \"Degree\": \"pos\"},\n",
    "    \"RBR\":      {POS: ADV, \"Degree\": \"comp\"},\n",
    "    \"RBS\":      {POS: ADV, \"Degree\": \"sup\"},\n",
    "    \"RP\":       {POS: PART},\n",
    "    \"SYM\":      {POS: SYM},\n",
    "    \"TO\":       {POS: PART, \"PartType\": \"inf\", \"VerbForm\": \"inf\"},\n",
    "    \"UH\":       {POS: INTJ},\n",
    "    \"VB\":       {POS: VERB, \"VerbForm\": \"inf\"},\n",
    "    \"VERB\":     {POS: VERB},\n",
    "    \"VBD\":      {POS: VERB, \"VerbForm\": \"fin\", \"Tense\": \"past\"},\n",
    "    \"VBG\":      {POS: VERB, \"VerbForm\": \"part\", \"Tense\": \"pres\", \"Aspect\": \"prog\"},\n",
    "    \"VBN\":      {POS: VERB, \"VerbForm\": \"part\", \"Tense\": \"past\", \"Aspect\": \"perf\"},\n",
    "    \"VBP\":      {POS: VERB, \"VerbForm\": \"fin\", \"Tense\": \"pres\"},\n",
    "    \"VBZ\":      {POS: VERB, \"VerbForm\": \"fin\", \"Tense\": \"pres\", \"Number\": \"sing\", \"Person\": 3},\n",
    "    \"WDT\":      {POS: ADJ, \"PronType\": \"int|rel\"},\n",
    "    \"WP\":       {POS: NOUN, \"PronType\": \"int|rel\"},\n",
    "    \"WP$\":      {POS: ADJ, \"Poss\": \"yes\", \"PronType\": \"int|rel\"},\n",
    "    \"WRB\":      {POS: ADV, \"PronType\": \"int|rel\"},\n",
    "    \"SP\":       {POS: SPACE},\n",
    "    \"ADD\":      {POS: X},\n",
    "    \"NFP\":      {POS: PUNCT},\n",
    "    \"GW\":       {POS: X},\n",
    "    \"XX\":       {POS: X},\n",
    "    \"BES\":      {POS: VERB},\n",
    "    \"HVS\":      {POS: VERB},\n",
    "    \"X\":        {POS:X}\n",
    "}\n",
    "\n",
    "corpus = nltk_corpus('brown')\n",
    "DATA = np.array(corpus.tagged_sents(tagset='universal'))\n",
    "vocab = Vocab(tag_map = TAG_MAP)\n",
    "tagger = Tagger(vocab)\n",
    "# for i in range(25):\n",
    "#     print i\n",
    "#     for sent in DATA:\n",
    "#         words, tags = zip(*sent)\n",
    "#         doc = Doc(vocab, words = words)\n",
    "#         gold = GoldParse(doc, tags=tags)\n",
    "#         tagger.update(doc, gold)\n",
    "#     random.shuffle(DATA)\n",
    "# tagger.model.end_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "#question answering part 1\n",
    "def merge_PROPN_chunk(matcher, doc, i, matches):\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge() \n",
    "        \n",
    "PROPN_PATTERN = [\n",
    "            {POS:u'PROPN','OP':'+'}   \n",
    "          ]\n",
    "    \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity(u'PROPN_CHUNK', on_match = merge_PROPN_chunk)\n",
    "matcher.add_pattern(u\"PROPN_CHUNK\", PROPN_PATTERN,  label=u'PROPN_CHUNK')\n",
    "\n",
    "def merge_ents(doc):\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "\n",
    "def post_process(doc):\n",
    "    matcher(doc)\n",
    "    merge_ents(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "def get_answer_type(doc):\n",
    "    post_process(doc)\n",
    "    tags = ['WDT','WP','WRB']\n",
    "    \n",
    "    question_type_flags = filter(lambda token: token.tag_ in tags, doc)\n",
    "    \n",
    "    if len(question_type_flags)==0:\n",
    "        warn(\"No question tag was found in request\")\n",
    "        return False\n",
    "    \n",
    "    elif len(question_type_flags) > 1:\n",
    "        warn(\"Multiple question tags found in request, resolving using parse\")\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        tag = question_type_flags[0]\n",
    "        answer_requirements = get_answer_requirements(tag)\n",
    "        return answer_requirements\n",
    "    \n",
    "def get_answer_requirements(token):\n",
    "    if token.tag_ == 'WRB':\n",
    "        if token.lower_ == 'where':\n",
    "            #Where was Star Wars Filmed\n",
    "            return ['LOCATION']\n",
    "        elif token.lower_ == 'when':\n",
    "            #When was Star Wars Filmed\n",
    "            return ['DATE']\n",
    "        elif token.lower_ == 'how':\n",
    "            #How much did Star Wars make?\n",
    "            if token.nbor().lower_ in ('much', 'many'):\n",
    "                return ['QUANTITY']\n",
    "\n",
    "            #How old is star wars?\n",
    "            elif token.nbor().lower_ in ('long', 'old'):\n",
    "                return ['DURATION']\n",
    "            else:\n",
    "                return False\n",
    "        elif token.lower() == 'whom':\n",
    "            #Whom did you see?\n",
    "            return ['PERSON','ORG']      \n",
    "        else:\n",
    "            return False\n",
    "    elif token.tag_ == 'WP':\n",
    "        #Asking for Identity\n",
    "        if token.lower_ in ('who', 'whose'):\n",
    "            #Who directed Star Wars?\n",
    "            return ['PERSON','ORG']\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #What is Star Wars\n",
    "            return False \n",
    "        else: \n",
    "            return False\n",
    "    elif token.tag_ == 'WDT':\n",
    "        #asking for a choice among options\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #which Star Wars did you like best?\n",
    "            return [token.nbor().lower_] #return neighbor\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False    \n",
    "    \n",
    "    \n",
    "def info(string):\n",
    "    return map(lambda x: (x.orth_, x.tag_, x.pos_), nlp(unicode(string)))\n",
    "\n",
    "get_answer_type(nlp(u'When was Star Wars released?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "import zipfile\n",
    "from ds_api.cache_utils import load_cache, s3_key\n",
    "from spacy.en import English\n",
    "\n",
    "def merge_PROPN_chunk(matcher, doc, i, matches):\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge() \n",
    "\n",
    "def custom_matcher(doc):\n",
    "    matcher = Matcher(doc.vocab)\n",
    "    matcher.add_entity(u'PROPN_CHUNK', on_match=merge_PROPN_chunk)\n",
    "    matcher.add_pattern(u\"PROPN_CHUNK\", verb_pattern,  label=u'PROPN_CHUNK')\n",
    "    return matcher(doc) \n",
    "\n",
    "PROPN_PATTERN = [\n",
    "            {POS:u'NN', 'OP':'*'},\n",
    "            {POS:u'NNP', 'OP':'*'},\n",
    "            {POS:u'PROPN','OP':'+'},\n",
    "            {POS:u'NN', 'OP':'*'},\n",
    "            {POS:u'NNP', 'OP':'*'}    \n",
    "          ]\n",
    "       \n",
    "        \n",
    "path = 'spacy-resources/spacy_files/en-1.1.0/'\n",
    "if os.path.exists(path):\n",
    "    nlp = spacy.en.English(path=path, create_pipeline = create_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "scrolled": false,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.attrs import POS\n",
    "from spacy.matcher import Matcher\n",
    "reuters = nltk_reader('abc', limit=100)\n",
    "\n",
    "def merge_PROPN_chunk(matcher, doc, i, matches):\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge() \n",
    "\n",
    "PROPN_PATTERN = [\n",
    "            {POS:u'PROPN','OP':'+'}\n",
    "          ]\n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity(u'PROPN_CHUNK', on_match = merge_PROPN_chunk)\n",
    "matcher.add_pattern(u\"PROPN_CHUNK\", PROPN_PATTERN,  label=u'PROPN_CHUNK')\n",
    "\n",
    "\n",
    "proper_nouns = Counter()\n",
    "\n",
    "for doc in nlp.pipe(reuters, n_threads=4):\n",
    "    match_idx = matcher(doc)\n",
    "    matches = map(lambda x: doc[x[2]:x[3]].orth_, match_idx)\n",
    "    proper_nouns.update(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "original_text = u'I was able to find the paper by using the Google search engine.'\n",
    "mangled_text = u'I was goble to find the doomple by greping the Dongle search engine.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "#grab sentences from SpaCy\n",
    "original_sentence = list(nlp(original_text).sents)[0]\n",
    "mangled_sentence = list(nlp(mangled_text).sents)[0]\n",
    "\n",
    "\n",
    "\n",
    "parts_of_speech = {}\n",
    "\n",
    "#iterate over each sentence, grabbing the tagged POS\n",
    "for idx in range(len(original_sentence)):\n",
    "    original_word, mangled_word = original_sentence[idx], mangled_sentence[idx]\n",
    "    \n",
    "    record = {\n",
    "             'original word': original_word\n",
    "             , 'mangled word': mangled_word\n",
    "             , 'original pos':original_word.pos_\n",
    "             , 'mangled pos':mangled_word.pos_\n",
    "    }\n",
    "    \n",
    "    parts_of_speech[idx] = record\n",
    "    \n",
    "    \n",
    "#present results in a table\n",
    "columns = ['original word','mangled word','original pos', 'mangled pos']\n",
    "pd.DataFrame(parts_of_speech).T[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "#this example uses shows the frequency of tokens with multiple\n",
    "#parts of speech\n",
    "\n",
    "#grab the brown corpus\n",
    "fileids = brown.fileids()\n",
    "doc_iter = (\" \".join([\" \".join(j) for j in brown.sents(fileid)]) for fileid in fileids)\n",
    "\n",
    "#create a dictionary of sets\n",
    "d = defaultdict(set)\n",
    "\n",
    "for doc in nlp.pipe(doc_iter, n_threads=4):\n",
    "    for token in doc:\n",
    "        #add the token's part of speech to the lexeme's set\n",
    "        d[token.lemma_].add(token.pos_)\n",
    "\n",
    "#grab the number of unique POSs per lexeme\n",
    "lengths = map(lambda key: len(d[key]), d) \n",
    "c = Counter(lengths)\n",
    "N = float(sum(c.values()))\n",
    "\n",
    "#plot\n",
    "ax = pd.Series(c).map(lambda x: x / N).plot(kind = 'bar')\n",
    "ax.set_xlabel(\"Unique Parts of Speech in Brown Corpus\")\n",
    "ax.set_ylabel(\"Percent of Unique Words in Vocabulary\")\n",
    "ax.grid(axis='x')\n",
    "ax.set_title(\"20% of Tokens in Brown Corpus Have Multiple Unique POS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "print d['down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n",
    "\n",
    "matcher.add_entity('VERB_PHRASE', on_match=merge_phrases)\n",
    "matcher.add_pattern('VERB_PHRASE', verb_pattern)\n",
    "# doc = Doc(matcher.vocab, words=[u'Google', u'Now', u'is', u'being', u'rebranded'])\n",
    "# matcher(doc)\n",
    "# print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "import zipfile\n",
    "from ds_api.cache_utils import load_cache, s3_key\n",
    "from spacy.en import English\n",
    "\n",
    "def merge_verb_chunk(matcher, doc, i, matches):\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge() \n",
    "\n",
    "def custom_matcher(doc):\n",
    "    matcher = Matcher(doc.vocab)\n",
    "    matcher.add_entity(u'VERB_CHUNK', on_match=merge_verb_chunk)\n",
    "    matcher.add_pattern(u\"VERB_CHUNK\", verb_pattern,  label=u'VERB_CHUNK')\n",
    "    return matcher(doc) \n",
    "        \n",
    "def create_pipeline(nlp):\n",
    "    return (nlp.tagger, nlp.parser, nlp.matcher, nlp.entity, custom_matcher, custom_matcher)\n",
    "\n",
    "verb_pattern = [\n",
    "            {POS:u'AUX', 'OP':'*'},\n",
    "            {POS:u'ADV', 'OP':'*'},\n",
    "            {POS:u'VERB','OP':'+'}\n",
    "          ]\n",
    "       \n",
    "        \n",
    "path = 'spacy-resources/spacy_files/en-1.1.0/'\n",
    "if os.path.exists(path):\n",
    "    nlp = spacy.en.English(path=path, create_pipeline = create_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "text = u'I could hardly believe that the Google search engine was that good'\n",
    "doc = nlp(text)\n",
    "list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "import zipfile\n",
    "from ds_api.cache_utils import load_cache, s3_key\n",
    "from spacy.en import English\n",
    "def get_spacy_parser(out_dir = 'spacy-resources'):\n",
    "    load_cache('spacy-english.zip', 'spacy-english.zip')\n",
    "    path_to_zip_file = 'spacy-english.zip'\n",
    "    zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
    "    zip_ref.extractall(out_dir)\n",
    "    zip_ref.close()    \n",
    "    english_directory = '{}/spacy_files/en-1.1.0/'.format(out_dir)\n",
    "    parser = English(path=english_directory)\n",
    "    return parser\n",
    "if os.path.exists('spacy-resources/spacy_files/en-1.1.0/')\n",
    "\n",
    "\n",
    "def merge_verb_phrase(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label_=label, pos='VERB')\n",
    "        \n",
    "verb_pattern = [\n",
    "            {POS:'AUX', 'OP':'*'},\n",
    "            {POS:'ADV', 'OP':'*'},\n",
    "            {POS:'VERB','OP':'+'},\n",
    "            {POS:'AUX', 'OP':'*'},\n",
    "            {POS:'ADV', 'OP':'*'},\n",
    "            {POS:'VERB','OP':'*'}\n",
    "          ]\n",
    "\n",
    "def custom_matcher(doc):\n",
    "    matcher = Matcher(doc.vocab)\n",
    "    matcher.add_entity('VERB_PHRASE', on_match=merge_verb_phrase)\n",
    "    matcher.add_pattern(\"VERB_PHRASE\", verb_pattern,  label='VERB_PHRASE')\n",
    "    return matcher(doc)\n",
    "\n",
    "\n",
    "\n",
    "#matcher = Matcher(nlp.vocab)\n",
    "#matcher.add_entity('VERB_PHRASE', on_match=merge_verb_phrase)\n",
    "#matcher.add_pattern(\"VERB_PHRASE\", verb_pattern,  label='VERB_PHRASE')\n",
    "\n",
    "def merge_texts(question):\n",
    "    for chunk in list(question.noun_chunks):\n",
    "        chunk.merge()\n",
    "    for ent in list(question.ents):\n",
    "        ent.merge()\n",
    "\n",
    "doc = nlp(u'I could hardly believe it')\n",
    "matches = matcher(doc)\n",
    "\n",
    "print map(lambda x: x.pos_, doc)\n",
    "\n",
    "print\n",
    "print list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "d = nlp(u'You would tell me if you could help me')\n",
    "print map(lambda x: x.pos_, d)\n",
    "print\n",
    "matcher(d)\n",
    "\n",
    "\n",
    "print map(lambda x: x.pos_, d)\n",
    "print\n",
    "print list(d.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "spacy.en.English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from ds_api.cache_utils import load_cache, s3_key\n",
    "from spacy.en import English\n",
    "def get_spacy_parser(out_dir = 'spacy-resources'):\n",
    "    load_cache('spacy-english.zip', 'spacy-english.zip')\n",
    "    path_to_zip_file = 'spacy-english.zip'\n",
    "    zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
    "    zip_ref.extractall(out_dir)\n",
    "    zip_ref.close()    \n",
    "    english_directory = '{}/spacy_files/en-1.1.0/'.format(out_dir)\n",
    "    parser = English(path=english_directory)\n",
    "    return parser\n",
    "\n",
    "\n",
    "nlp = get_spacy_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "nltk.download('conll2000')\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['VP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "s = train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "!sudo pip install spacy -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "fileids = conll2000.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "fileids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "#https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv\n",
    "url = 'https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv?token=AGz3FdVBO7kTQsMgkoMrqdZlTI1Tbfmiks5YXX67wA%3D%3D'\n",
    "s=requests.get(url).content\n",
    "c=pd.read_csv(io.StringIO(s.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "zip(words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "tagger2 = Tagger(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(u'I am bob how are you?')\n",
    "tagger2(doc)\n",
    "map(lambda x: x.pos_, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "scrolled": true,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "!sudo pip install cufflinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECRET_ENV_GITHUB_USERNAME=johnchuckcase\r\nSECRET_ENV_TEST_ENV=test\r\nSECRET_ENV_AWS_ACCESS_KEY_SHARED=AKIAJ4Z2PMTE5XZJ3SLQ\r\nSECRET_ENV_REDSHIFT_PASS=&$&CAEcI$l3p@Xtr\r\nSECRET_ENV_AWS_ACCESS_KEY_EBTH=AKIAJAHDMEITUE6GQD7Q\r\nSECRET_ENV_CUSTOMER_EBTH=ebth\r\nSECRET_ENV_DS_EBTH_RS_PASSWORD=O630ABBKfFGbZub@\r\nSECRET_ENV_REDSHIFT_HOST=data-science.cex3rfvdw0wv.us-west-2.redshift.amazonaws.com\r\nSECRET_ENV_DS_EBTH_RS_HOST=data-science.cex3rfvdw0wv.us-west-2.redshift.amazonaws.com\r\nSECRET_ENV_REDSHIFT_USER=nnadkarni\r\nSECRET_ENV_CUSTOMER=internal\r\nSECRET_ENV_DS_EBTH_RS_USER=jrgauthier\r\nSECRET_ENV_GITHUB_PASSWORD=green1994\r\nSECRET_ENV_REDSHIFT_PORT=5439\r\nSECRET_ENV_AWS_SECRET_KEY_SHARED=XDu7pB+dzf1iA821dtThMIl7GY4yHwT24vHjYZ8O\r\nSECRET_ENV_DS_EBTH_RS_PORT=5439\r\nSECRET_ENV_AWS_SECRET_KEY_EBTH=x2ertIFJBA+unDvCKqEGxC5WPnx6tkpzi4iSFdVL\r\n"
     ]
    }
   ],
   "source": [
    "SECRET_ENV_AARON_PLOTLY_USERNAME = 'aikramer2'\n",
    "SECRET_ENV_AARON_PLOT_API_KEY = 'pQ4lbj8qWkPklCcowaHZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "SECRET_ENV_AARON_PLOTLY_USERNAME"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "pygments_lexer": "ipython2",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "2.7.12",
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python"
  },
  "_datascience": {
   "notebookId": 749.0
  },
  "kernelspec": {
   "language": "python",
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
