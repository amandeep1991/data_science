{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<h3>Basic Recipe for Training a POS Tagger with SpaCy</h3>\n<ol>\n<li id=\"loaddatatitle\"><a href=\"#-Load-Data-\">Load Data </a>\n<ol><li>We'll be using a sample from Web Treebank corpus, in ConllX format</ol>\n<li><a href=\"#Prepare-Environment-for-New-Model\">Prepare environment for a new model</a>\n<ol><li>New model directory, with tagger and parser subdirectories. (Ensure you have permission)</ol>\n<li><a href=\"#Build-a-Vocabulary\">Build a vocabulary</a>\n\n<ol>\n<li>We are just going to load the default English Vocabulary\n<li>Defines how we get attributes (like suffix) from a token string\n<li>Includes brown cluster data on lexemes, we'll use as a feature for the parser\n</ol>\n<li> <a href=\"#Build-a-Tagger\">Build a Tagger</a>\n<ol><li>Ensure tagmap is provided if needed</ol>\n<ol><li>Which features should be used to train tagger?</ol>\n<li><a href=\"#Train-Tagger\"> Train Tagger</a>\n<ol><li>Averaged Perceptron algorithm\n<li>For each epoch: \n<ol><li>For each document in training data:\n<ol><li>For each sentence in document:\n<ol>\n    <li>Create document with sentence words (tagger not yet applied)\n    <li>Create GoldParse object with annotated labels\n    <li>Apply the tagger to the document to get predictions\n    <li>Update the tagger with GoldParse, Document (actual v predicted)\n</ol>\n</ol>\n<li> Score predictions on validation set\n</ol>\n</ol>\n<li><a href=\"#Save-Tagger\">Save Tagger</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<h3> Load Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jupyter/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 202 malformed lines\nSkipped 1 malformed lines\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from spacy.syntax.arc_eager import PseudoProjectivity\n",
    "def sent_iter(conll_corpus):\n",
    "    for _, doc_sents in conll_corpus:\n",
    "        for (ids, words, tags, heads, deps, ner), _ in doc_sents:\n",
    "            yield ids, words, tags, heads, deps, ner\n",
    "            \n",
    "def read_conllx(text):\n",
    "    bad_lines = 0\n",
    "    for sent in text.strip().split('\\n\\n'):       \n",
    "        lines = sent.strip().split('\\n')\n",
    "        if lines:\n",
    "            while lines[0].startswith('#'):\n",
    "                lines.pop(0)\n",
    "            tokens = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    id_, word, lemma, tag, pos, morph, head, dep, _1, _2 = line.split()\n",
    "                    if '-' in id_:\n",
    "                        continue\n",
    "                    id_ = int(id_) - 1\n",
    "                    head = (int(head) - 1) if head != '0' else id_\n",
    "                    dep = 'ROOT' if dep == 'root' else dep\n",
    "                    tokens.append((int(id_), unicode(word), unicode(pos), int(head), unicode(dep), 'O'))\n",
    "                except:\n",
    "                    bad_lines += 1\n",
    "            tuples = [list(t) for t in zip(*tokens)]\n",
    "            yield (None, [[tuples, []]])\n",
    "    print(\"Skipped {} malformed lines\".format(bad_lines))\n",
    "\n",
    "            \n",
    "            \n",
    "def LoadData(url, make_projective = False):\n",
    "    conll_string = requests.get(url).content\n",
    "    sents = list(read_conllx(conll_string))\n",
    "    if make_projective:\n",
    "        sents = PseudoProjectivity.preprocess_training_data(sents)\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "train_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-train.conllu'\n",
    "test_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-test.conllu'\n",
    "train_sents = LoadData(train_url)\n",
    "test_sents = LoadData(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus metadata\n\nNumber of Sentences: 12543\nNumber of Unique Tags: 50\nUnique Tags: set([u'PRP$', u'VBG', u'VBD', u'NFP', u'``', u'VBN', u'POS', u\"''\", u'VBP', u'WDT', u'JJ', u'WP', u'VBZ', u'DT', u'RP', u'$', u'NN', u'FW', u',', u'.', u'TO', u'PRP', u'RB', u'-LRB-', u':', u'NNS', u'HYPH', u'GW', u'XX', u'VB', u'WRB', u'CC', u'LS', u'PDT', u'RBS', u'RBR', u'CD', u'ADD', u'AFX', u'EX', u'IN', u'WP$', u'MD', u'NNPS', u'-RRB-', u'JJS', u'JJR', u'SYM', u'UH', u'NNP'])\n"
     ]
    }
   ],
   "source": [
    "sent_counter = 0\n",
    "unique_tags = set()\n",
    "for ids, words, tags, heads, deps, ner in sent_iter(train_sents):\n",
    "    unique_tags.update(tags)\n",
    "    sent_counter += 1\n",
    "doc_counter = len(train_sents)\n",
    "print(\"Training corpus metadata\")\n",
    "print\n",
    "print(\"Number of Sentences: {}\".format(sent_counter))\n",
    "print(\"Number of Unique Tags: {}\".format(len(unique_tags)))\n",
    "print(\"Unique Tags: {}\".format(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Prepare Environment for New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "def prepare_environment_for_new_tagger(model_path, tagger_path):\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "    if not tagger_path.exists():\n",
    "        tagger_path.mkdir()\n",
    "        \n",
    "data_dir = spacy.en.get_data_path()\n",
    "model_dir = data_dir / 'en-1.1.0'\n",
    "tagger_dir = model_dir / 'custom-pos-tagger'\n",
    "prepare_environment_for_new_tagger(model_dir, tagger_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "def build_vocab(model_dir, vec_path = None, lexeme_path = None):\n",
    "    vocab = Vocab.load(model_dir)\n",
    "    if lexeme_path:\n",
    "        vocab.load_lexemes(lexeme_path)\n",
    "    if vec_path:\n",
    "        vocab.load_vectors_from_bin_loc(vec_path)\n",
    "        \n",
    "    return vocab\n",
    "    \n",
    "lexeme_path = model_dir / 'vocab' / 'lexemes.bin'\n",
    "vocab = build_vocab(model_dir, lexeme_path=lexeme_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Value for 'He': 126\n"
     ]
    }
   ],
   "source": [
    "#test clusters are available\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "doc = Doc(vocab, words=[u'He',u'ate',u'pizza',u'.'])\n",
    "print(\"Cluster Value for '{}': {}\".format(*[doc[0], doc[0].cluster]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Build a Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import Tagger\n",
    "from spacy.tagger import *\n",
    "\n",
    "features = [\n",
    "    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),    #current word attributes   \n",
    "    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),                        #-1 word attributes \n",
    "    (P2_pos,),(P2_cluster,),(P2_flags,),                                     #-2 word attributes  \n",
    "    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),                       #+1 word attributes       \n",
    "    (N2_orth,),(N2_cluster,),(N2_flags,),                                    #+2 word attributes \n",
    "    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth) #combination attributes \n",
    "]\n",
    "\n",
    "features = spacy.en.English.Defaults.tagger_features\n",
    "tag_map = spacy.en.tag_map\n",
    "statistical_model = spacy.tagger.TaggerModel(features)\n",
    "tagger = Tagger(vocab, tag_map=tag_map, statistical_model = statistical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Train Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:\t\tPOS Tag Accuracy\nPretraining:\t\t0.000\n0:\t\t\t87.655\n1:\t\t\t89.122\n2:\t\t\t91.250\n3:\t\t\t91.110\n4:\t\t\t91.453\n5:\t\t\t91.851\n6:\t\t\t92.545\n7:\t\t\t92.302\n8:\t\t\t92.246\n9:\t\t\t91.843\n"
     ]
    }
   ],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "import random\n",
    "\n",
    "\n",
    "def score_model(vocab, tagger, gold_docs, verbose=False):\n",
    "    scorer = Scorer()\n",
    "    for _, gold_doc in gold_docs:\n",
    "        for (ids, words, tags, heads, deps, entities), _ in gold_doc:\n",
    "            doc = Doc(vocab, words=map(unicode,words))\n",
    "            tagger(doc)\n",
    "            gold = GoldParse(doc, tags=tags)\n",
    "            scorer.score(doc, gold, verbose=verbose)\n",
    "    return scorer  \n",
    "\n",
    "\n",
    "def train(tagger, vocab, train_sents, test_sents, model_dir, n_iter=20, seed = 0, feat_set = u'basic'):\n",
    "    scorer = score_model(vocab, tagger, test_sents)\n",
    "    print('%s:\\t\\t%s' % (\"Iteration\", \"POS Tag Accuracy\"))            \n",
    "    print('%s:\\t\\t%.3f' % (\"Pretraining\", scorer.tags_acc))        \n",
    "    \n",
    "    #TRAINING STARTS HERE\n",
    "    for itn in range(n_iter):\n",
    "        for ids, words, tags, heads, deps, ner in sent_iter(train_sents):\n",
    "            doc = Doc(vocab, words=map(unicode,words))\n",
    "            gold = GoldParse(doc, tags=tags, heads=heads, deps=deps)\n",
    "            tagger(doc)\n",
    "            tagger.update(doc, gold)\n",
    "        random.shuffle(train_sents)\n",
    "        scorer = score_model(vocab, tagger, test_sents)\n",
    "        print('%d:\\t\\t\\t%.3f' % (itn, scorer.tags_acc))\n",
    "    return tagger\n",
    "trained_tagger = train(tagger, vocab, train_sents, test_sents, model_dir, n_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Save Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "        \n",
    "ensure_dir(tagger_dir)\n",
    "trained_tagger.model.dump(str(tagger_dir / 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Notes\n<br>\n1. Spacy will be rolling out a neural network model soon!\n<br>\n<br>\n2. Checkout Speech and Language Processing by Daniel Jurafsky and James H. Martin\n<br>\n<br>\n3. Next section: Vector space models for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "pygments_lexer": "ipython2",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "2.7.12",
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python"
  },
  "_datascience": {
   "notebookId": 750.0
  },
  "kernelspec": {
   "language": "python",
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
