{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!pip install spacy\n",
    "!python -m spacy.en.download all\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install theano\n",
    "!pip install keras\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Embedding, Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "import spacy\n",
    "nlp = spacy.load('en', parse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='test')\n",
    "texts = map(lambda x: x.replace(\"\\n\",\"\"), newsgroups_train.data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pick_value(token):\n",
    "    if token.lemma_ in u'the it be for a of and but to in'.split():\n",
    "        return \"\"\n",
    "    if token.is_space:\n",
    "        return \"\"\n",
    "    elif token.like_url:\n",
    "        return \"URL\" + token.whitespace_\n",
    "    elif token.like_email:\n",
    "        return \"EMAIL\" + token.whitespace_    \n",
    "    elif token.like_num:\n",
    "        return \"NUM\" + token.whitespace_  \n",
    "    elif token.is_punct:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return token.lemma_ + token.whitespace_\n",
    "    \n",
    "tokenized_documents = []\n",
    "for doc in nlp.pipe(texts, n_threads = 8):\n",
    "    tokens = [pick_value(token) for token in doc]\n",
    "    tokenized_documents.append(tokens)\n",
    "    \n",
    "processed_documents = [\" \".join(doc) for doc in tokenized_documents]    \n",
    "tokenized_documents = [doc.split() for doc in processed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in tokenized_documents:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "tokenized_documents = [[token for token in text if frequency[token] > 5] for text in tokenized_documents]        \n",
    "processed_documents = [\" \".join(doc) for doc in tokenized_documents]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def flatten(L):\n",
    "    return [item for sublist in L for item in sublist]\n",
    "    \n",
    "def docsToIDMatrix(tokenized_documents, window = 3):\n",
    "    \n",
    "    processed_documents = [\" \".join(doc) for doc in tokenized_documents]  \n",
    "    \n",
    "    #get statistics\n",
    "    n_docs = len(tokenized_documents)\n",
    "    all_words = flatten(tokenized_documents)\n",
    "    unique_words = list(set(all_words))\n",
    "    n_words = len(unique_words)\n",
    "\n",
    "    #create token ids\n",
    "    token_2_id = {j:i for i, j in enumerate(unique_words)}\n",
    "    id_2_token = {i:j for i, j in enumerate(unique_words)}\n",
    "\n",
    "    docs_as_ids = [[token_2_id[token] for token in doc ] for doc in tokenized_documents]\n",
    "    \n",
    "    return docs_as_ids, id_2_token, token_2_id\n",
    "\n",
    "\n",
    "docs_as_ids, id_2_token, token_2_id = docsToIDMatrix(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(tokenized_documents)\n",
    "model.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(L):\n",
    "    return [item for sublist in L for item in sublist]\n",
    "\n",
    "\n",
    "#get statistics\n",
    "n_docs = len(processed_documents)\n",
    "all_words = flatten(tokenized_documents)\n",
    "unique_words = list(set(all_words))\n",
    "n_words = len(unique_words)\n",
    "\n",
    "#create token ids\n",
    "token_2_id = {j:i for i, j in enumerate(unique_words)}\n",
    "id_2_token = {i:j for i, j in enumerate(unique_words)}\n",
    "\n",
    "docs_as_ids = [[token_2_id[token] for token in doc ] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Questions for Learning Word Embedding Models:\n",
    "\n",
    "1) What is a suitable \"context\"?\n",
    "\n",
    "* Larger contexts $\\rightarrow$ higher level relationships; thematically related\n",
    "    * E.g Documents, paragraphs\n",
    "* Smaller contexts $\\rightarrow$ lower level relationships; synonyms\n",
    "    * E.g Neighboring words, syntactic dependencies    \n",
    "* Should we employ weights?\n",
    "    * information-theoretic: tfidf, local/global\n",
    "    * proximity-based\n",
    "\n",
    "2) Unsupervised or supervised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Code Example: term-document LSA with local/global weighting\n",
    "\n",
    "* Input: Term-Document Matrix\n",
    "* Frequency Scaling: Log\n",
    "* Document Frequency Scaling: Entropy\n",
    "* Criteria for embedding size: pick K dimensions that explain at least 70% of overall variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "def build_document_count_matrix(documents):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---------\n",
    "    documents: list of strings\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=5)\n",
    "    count_matrix = vectorizer.fit_transform(documents).todense()\n",
    "    \n",
    "    return vectorizer, count_matrix, vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "def get_scaled_weights(count_matrix):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---------\n",
    "    count_matrix: numpy matrix of dim (documents, terms)\n",
    "    \"\"\"\n",
    "    #number of words\n",
    "    V = count_matrix.shape[1]\n",
    "    \n",
    "    #sublinear scaled count of term\n",
    "    local_weights = np.log(count_matrix + 1)\n",
    "    \n",
    "    #entropy of term\n",
    "    global_weights = np.array(1 + np.multiply(count_matrix + 1,np.log(count_matrix + 1)).sum(axis=0) / np.log(V))\n",
    "    global_weights = global_weights.reshape(V)\n",
    "    \n",
    "    return local_weights / global_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer, count_matrix, vocabulary = build_document_count_matrix(processed_documents)\n",
    "N = len(vocabulary)  \n",
    "\n",
    "weights = get_scaled_weights(count_matrix)\n",
    "\n",
    "svd = TruncatedSVD(n_components=N-1)\n",
    "svd.fit_transform(weights)\n",
    "embeddings = svd.components_.T\n",
    "\n",
    "info_threshold = .7\n",
    "weight_info = np.cumsum(svd.explained_variance_ratio_)\n",
    "required_dims = np.where(weight_info < info_threshold)[0].max()\n",
    "k_embeddings = embeddings[:, :required_dims]\n",
    "\n",
    "print \"Original Vocab: {}\".format(N)\n",
    "print \"LSA embeddings dimension: {}\".format(k_embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Determine most similar words with cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute vector similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "def most_similar(embedding_space, vocabulary, word, n = 10):\n",
    "    assert word.lower() in vocabulary, \"Word needs to be in vocabulary\"\n",
    "    \n",
    "    word_idx = vocabulary.index(word)\n",
    "    word_vector = np.expand_dims(embedding_space[word_idx], axis=0)\n",
    "    \n",
    "    sim_matrix = cosine_similarity(X = embedding_space, Y = word_vector).reshape(embedding_space.shape[0])\n",
    "    most_similar = np.argsort(sim_matrix)[::-1][:n]\n",
    "\n",
    "    for idx in most_similar:\n",
    "        yield vocabulary[idx]\n",
    "        \n",
    "for i in most_similar(k_embeddings, vocabulary, 'bank'):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Neural Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "class WordEmbeddingModel(object):\n",
    "    \n",
    "    \"\"\"WARNING: INEFFICIENT! Just for demonstration purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenized_documents, context_window = 3, embedding_size = 100, n_epochs = 5, learning_rate = 0.1, verbose_mode = 'info'):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose_mode = verbose_mode\n",
    "        self.logger = self._build_logger()\n",
    "        self.window = context_window\n",
    "        \n",
    "        #document processing\n",
    "        self.docs_as_ids, self.id_2_token, self.token_2_id = docsToIDMatrix(tokenized_documents)\n",
    "        flattened_docs = flatten(self.docs_as_ids)\n",
    "        self.vocab_size = len(set(flattened_docs))\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_seq_length = 2 * self.window\n",
    "        self.n_obs = float(len(flattened_docs))\n",
    "        \n",
    "        #initialize data structures for weights\n",
    "        self.projection_matrix = np.random.randn(self.vocab_size, self.embedding_size)\n",
    "        self.prediction_matrix = np.random.randn(self.embedding_size, self.vocab_size)\n",
    "        self.input_matrix = np.diag(np.ones(self.vocab_size)) #convert ids to one hot encodings\n",
    "\n",
    "    def _build_logger(self):\n",
    "        if self.verbose_mode == 'info':\n",
    "            self.verbose = logging.INFO\n",
    "        elif self.verbose_mode == 'debug':\n",
    "            self.verbose = logging.DEBUG\n",
    "        else:\n",
    "            self.verbose = logging.INFO\n",
    "        logger = logging.Logger('w2v-log')\n",
    "        ch = logging.StreamHandler(sys.stdout)\n",
    "        ch.setLevel(self.verbose)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        return logger\n",
    "    \n",
    "    \n",
    "    def update_step(self, word, context):\n",
    "        \n",
    "        #feed forward\n",
    "        \n",
    "        projection = np.mean(self.projection_matrix[context], axis=0)\n",
    "        prediction = softmax(np.dot(projection, self.prediction_matrix))        \n",
    "        target_vector = self.input_matrix[word]\n",
    "        \n",
    "        #calculate delta for prediction weight matrix\n",
    "        output_delta = (prediction - target_vector).reshape(self.vocab_size, 1) # (vocab, 1)\n",
    "        prediction_delta = np.dot(projection.reshape(self.embedding_size, 1), output_delta.T) #(embbeding, vocab)\n",
    "    \n",
    "        #adjust prediction weight matrix\n",
    "        self.prediction_matrix = self.prediction_matrix - (self.learning_rate * prediction_delta)\n",
    "        hidden_delta = np.einsum('j,ij->i',output_delta.reshape(self.vocab_size), self.prediction_matrix)\n",
    "\n",
    "        for k in context:\n",
    "            self.projection_matrix[k] = self.projection_matrix[k] - (self.learning_rate * hidden_delta)/float(len(context))\n",
    "\n",
    "                        \n",
    "    def train(self):\n",
    "        for epoch in xrange(self.n_epochs):\n",
    "            self.logger.info(\"Epoch %s\" % epoch)\n",
    "            \n",
    "            for obs_i, sentence in enumerate(self.docs_as_ids):\n",
    "                self.logger.debug(\"Epoch %s Obs %s\" % (epoch, obs_i))\n",
    "                \n",
    "                text_length = len(sentence)\n",
    "                for token_idx in range(text_length):\n",
    "                    word = sentence[token_idx] #word as absolute id\n",
    "                    context_idx = range(max(token_idx - self.window, 0), min(token_idx + self.window, text_length-1)) #context as relative ids\n",
    "                    context = [sentence[i] for i in context_idx] #content as absolute ids\n",
    "                    self.update_step(word, context)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    def most_similar(self, word,n=10):\n",
    "        assert word.lower() in self.token_2_id.keys(), \"%s not found in vocabulary\" % word\n",
    "            \n",
    "        idx = self.token_2_id[word.lower()]\n",
    "        vec = self.projection_matrix[idx]\n",
    "        sims = cosine_similarity(X=self.projection_matrix, Y=vec)\n",
    "        \n",
    "        sims = sims.reshape(sims.shape[0])\n",
    "        \n",
    "        most_sims = np.argsort(sims)[::-1][:n]\n",
    "        words = [self.id_2_token[i] for i in most_sims]        \n",
    "        return words\n",
    "        \n",
    "#print prediction_delta.shape, self.prediction_matrix.shape\n",
    "m = WordEmbeddingModel(tokenized_documents, verbose_mode='info')\n",
    "m.train()\n",
    "m.most_similar(u'curious')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### lsa with word context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###lsa with word context\n",
    "#create empty matrix for term context\n",
    "n_words = len(vocabulary)\n",
    "token_2_id = {j:i for i, j in enumerate(vocabulary)}\n",
    "id_2_token = {i:j for i, j in enumerate(vocabulary)}\n",
    "term_frequency = np.zeros((n_words, n_words))\n",
    "\n",
    "#set window size\n",
    "window = 5\n",
    "for text in tokenized_documents:\n",
    "    text = filter(lambda x: x in vocabulary, text)\n",
    "    text_length = len(text)\n",
    "    for token_idx in range(text_length):\n",
    "        row_index = token_2_id[text[token_idx]]\n",
    "        context_idx = (max(token_idx - window, 0), min(token_idx + window, text_length-1))\n",
    "        for idx in range(*context_idx):\n",
    "            column_index = token_2_id[text[idx]]\n",
    "            term_frequency[row_index, column_index] += 1           \n",
    "            \n",
    "#tfidf weighting\n",
    "context_frequency = np.log(((term_frequency > 1).astype(float)).sum(axis=1) + 1 / float(n_words))\n",
    "term_context_matrix = term_frequency / context_frequency\n",
    "\n",
    "\n",
    "#build low dimensional representation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "svd.fit(doc_context_matrix)\n",
    "embeddings = svd.components_.T\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "word_idx = unique_words.index('bank')\n",
    "row = sim_matrix[word_idx]\n",
    "most_similar = np.argsort(row)[::-1]\n",
    "\n",
    "for idx in most_similar[0:10]:\n",
    "    print unique_words[idx]"
   ]
  }
 ],
 "metadata": {
  "_datascience": {
   "notebookId": 758
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
