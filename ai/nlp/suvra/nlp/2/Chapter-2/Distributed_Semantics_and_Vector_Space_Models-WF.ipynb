{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/home/jupyter/site-packages/'\n",
    "sys.path.append(path)\n",
    "# !pip install theano\n",
    "# !pip install keras\n",
    "# !pip install nltk\n",
    "# !pip install plotly\n",
    "# import os\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from IPython.display import SVG, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "\n",
    "py.sign_in(os.environ['SECRET_ENV_AARON_PLOTLY_USERNAME'], os.environ['SECRET_ENV_AARON_PLOT_API_KEY'])\n",
    "offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en')\n",
    "\n",
    "text = u'Word vectors are fantastic!'\n",
    "doc = nlp(text)\n",
    "token = doc[1]\n",
    "print token.vector[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "average_of_token_vectors = np.mean([token.vector for token in doc],axis=0)\n",
    "document_vector = doc.vector\n",
    "assert all(average_of_token_vectors - document_vector == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "#grab word vectors for each word\n",
    "words = [u'cat',u'dog',u'man',u'woman']\n",
    "vectors = map(lambda word: nlp(word).vector, words)\n",
    "\n",
    "#create a dataframe of similarities\n",
    "similarities = cosine_similarity(vectors)\n",
    "similarity_matrix = pd.DataFrame(similarities, index = words, columns = words)\n",
    "\n",
    "\n",
    "data = [go.Heatmap( z=similarity_matrix.T.values.tolist()\n",
    "                   , colorscale='OrRd'\n",
    "                   ,x = words\n",
    "                   ,y = words\n",
    "                  )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Similarity of Word Vectors'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout = layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grab word vectors for each word\n",
    "words = [u'man',u'men',u'woman',u'women']\n",
    "vectors = map(lambda word: nlp(word).vector, words)\n",
    "man,men,woman,women = vectors\n",
    "attempted_women = men - man + woman\n",
    "\n",
    "\n",
    "\n",
    "#plural_men = nlp(u'men').vector - nlp(u'man').vector\n",
    "#plural_dogs = nlp(u'dogs').vector - nlp(u'dog').vector\n",
    "#plural = (plural_men + plural_dogs) / 2\n",
    "\n",
    "#vectors.append(plural + vectors[words.index('woman')])\n",
    "vectors.append(attempted_women)\n",
    "words.append('men - man + woman')\n",
    "\n",
    "similarities = cosine_similarity(np.array(vectors))\n",
    "similarity_matrix = pd.DataFrame(similarities, index = words, columns = words)\n",
    "\n",
    "data = [go.Heatmap( z=similarity_matrix.T.values.tolist()\n",
    "                   , colorscale='OrRd'\n",
    "                   ,x = words\n",
    "                   ,y = words\n",
    "                  )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Men - Man + Woman â‰ˆ Women'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout = layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "def corpus_to_x_y(corpus):\n",
    "    fileids = corpus.fileids()\n",
    "    tuples = map(lambda i: (\" \".join(corpus.words(i)), corpus.categories(i)[0]),fileids)\n",
    "    x, y = zip(*tuples)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "#prepare data for classification\n",
    "documents, categories = corpus_to_x_y(nltk_corpus('brown'))\n",
    "documents, categories = shuffle(documents, categories)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(categories)\n",
    "\n",
    "#Category Breakdown\n",
    "c = Counter(categories)\n",
    "for i in c:\n",
    "    print i, c[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classify using DTM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import log_loss \n",
    "from sklearn.metrics import classification_report, f1_score, precision_score\n",
    "\n",
    "f1_scores = {}\n",
    "models = {}\n",
    "losses = {}\n",
    "\n",
    "def train_and_validate(name, model_classes,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    if name not in f1_scores:\n",
    "        f1_scores[name] = {}\n",
    "        \n",
    "    if name not in models:\n",
    "        models[name] = {}\n",
    "        \n",
    "    if name not in losses:\n",
    "        losses[name] = {}        \n",
    "        \n",
    "    for model in model_classes:\n",
    "        model.fit(X_train, y_train)    \n",
    "        predictions = model.predict(X_test)\n",
    "        probabilities = model.predict_proba(X_test)\n",
    "    \n",
    "        losses[name][model.__module__] = log_loss(y_test, probabilities)\n",
    "        f1_scores[name][model.__module__] = f1_score(y_test, predictions, average = 'weighted')\n",
    "        models[name][model.__module__] = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create DTM\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "doc_train, doc_test, y_train, y_test = train_test_split(documents, y, test_size = .33)\n",
    "\n",
    "#create document term matrix with CountVectorizer\n",
    "Vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "#create training and testing DTM\n",
    "X_train_dtm = Vectorizer.fit_transform(doc_train).todense()\n",
    "X_test_dtm = Vectorizer.transform(doc_test).todense()\n",
    "\n",
    "print \"Shape of Document Term Matrix: {}\".format(X_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_classes = [LogisticRegression()  \n",
    "              , GaussianNB()\n",
    "              , SVC(kernel='linear', probability=True)\n",
    "              , RandomForestClassifier(n_estimators=100)\n",
    "              , DummyClassifier()]\n",
    "\n",
    "train_and_validate('DTM', model_classes, X_train_dtm, X_test_dtm, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_names = f1_scores['DTM'].keys()\n",
    "\n",
    "dtm_f1_trace = go.Bar(\n",
    "                    y=[f1_scores['DTM'][model] for model in model_names],\n",
    "                    x=model_names\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group', title='F1 Scores for Using Document Term Matrices'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[dtm_f1_trace], layout = layout)\n",
    "py.iplot(fig, filename='make-subplots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_doc_vec = np.array(map(lambda x: nlp(x, parse=False, entity=False).vector, doc_train))\n",
    "X_test_doc_vec = np.array(map(lambda x: nlp(x,parse=False, entity=False).vector, doc_test))\n",
    "\n",
    "print \"Shape of Document Vector Matrix: {}\".format(X_train_doc_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {
     "summary": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model_classes = [LogisticRegression(C=100.)  \n",
    "              , BernoulliNB()\n",
    "              , SVC(kernel='linear', probability=True, C=100.)\n",
    "              , RandomForestClassifier(n_estimators=100)\n",
    "              , DummyClassifier()]\n",
    "\n",
    "train_and_validate('WordVec', model_classes, X_train_doc_vec, X_test_doc_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_names = [i.__module__ for i in model_classes]\n",
    "\n",
    "dtm_f1_trace = go.Bar(\n",
    "                    y=[f1_scores['DTM'][model] for model in model_names],\n",
    "                    x=model_names,\n",
    "                    name = 'Document Term Matrix'\n",
    ")\n",
    "\n",
    "vect_f1_trace = go.Bar(\n",
    "                    y=[f1_scores['WordVec'][model] for model in model_names],\n",
    "                    x=model_names,\n",
    "                    name = 'Word Vectors'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group', title='F1 Scores for Using Document Term Matrices', yaxis=dict(title = 'F1 Score')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[dtm_f1_trace, vect_f1_trace], layout = layout)\n",
    "py.iplot(fig, filename='make-subplots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_doc = u\"President Obama enacted sanctions on Russia\"\n",
    "\n",
    "#convert to vectors for prediction\n",
    "dtm_vector = Vectorizer.transform([new_doc])\n",
    "doc_vector = nlp(new_doc).vector\n",
    "\n",
    "#predict using logistic model\n",
    "dtm_predictions = models['DTM']['sklearn.linear_model.logistic'].predict_proba(dtm_vector)\n",
    "docvec_predictions = models['WordVec']['sklearn.linear_model.logistic'].predict_proba(doc_vector)\n",
    "docvec_predictions = pd.Series(docvec_predictions.T.reshape(15,), encoder.classes_)\n",
    "dtm_predictions = pd.Series(dtm_predictions.T.reshape(15,), encoder.classes_)\n",
    "\n",
    "#common order for models\n",
    "order = docvec_predictions.sort_values().index.values\n",
    "\n",
    "\n",
    "#plot\n",
    "dtm_f1_trace = go.Bar(\n",
    "                    y=dtm_predictions.loc[order],\n",
    "                    x=order,\n",
    "                    name = 'DTM Predicted'\n",
    ")\n",
    "vect_f1_trace = go.Bar(\n",
    "                    y=docvec_predictions.loc[order],\n",
    "                    x=order,\n",
    "                    name = 'Word Vector Predicted'\n",
    ")\n",
    "layout = go.Layout(\n",
    "    barmode='group', title='Predicted Classes of \"%s\"' % new_doc, yaxis=dict(title = 'P(category)')\n",
    ")\n",
    "fig = go.Figure(data=[dtm_f1_trace, vect_f1_trace], layout = layout)\n",
    "py.iplot(fig, filename='make-subplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Extra: Convolutional NN using Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, LSTM, Dropout, Activation, Dense, Input, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "def docs_to_seqs(documents):\n",
    "\n",
    "    tokenized_sents = []\n",
    "    for doc in nlp.pipe(documents, parse = False, n_threads=8):\n",
    "        tokenized_sents.append([i.lemma for i in doc])\n",
    "        \n",
    "    return tokenized_sents\n",
    "\n",
    "\n",
    "def seq_to_ids_and_embeds(tokenized_sents, embedding_dim = 300):\n",
    "\n",
    "    n_docs = len(tokenized_sents)\n",
    "    max_len = max(map(len, tokenized_sents))\n",
    "    all_words = [item for sublist in tokenized_sents for item in sublist]   \n",
    "    n_words = len(set(all_words))\n",
    "    unique_words = list(set(all_words))\n",
    "    \n",
    "    \n",
    "    \n",
    "    embedding_matrix = np.zeros((n_words, embedding_dim))\n",
    "    embedding_count = 0\n",
    "    lexid_2_embed_matrix = {}\n",
    "\n",
    "    for lex_id in unique_words:\n",
    "        embedding_vector = nlp.vocab[lex_id].vector\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[embedding_count] = embedding_vector\n",
    "            lexid_2_embed_matrix[lex_id] = embedding_count\n",
    "            embedding_count += 1\n",
    "\n",
    "    keras_tokenized_sents = map(lambda sent: [lexid_2_embed_matrix[i] for i in sent],tokenized_sents)\n",
    "    return keras_tokenized_sents, embedding_matrix, max_len, n_words\n",
    "\n",
    "\n",
    "def conv_model_w_dropout(n_words, embedding_dim, embedding_matrix, max_len):\n",
    "    embedding_layer = Embedding(n_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False)\n",
    "    input_ = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(input_)\n",
    "    \n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(35)(x) \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    output_ = Dense(keras_train_y.shape[1], activation='softmax')(x)\n",
    "    model = Model(input_ ,output_)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "seqs = docs_to_seqs(documents)\n",
    "embedding_dim = 300\n",
    "keras_X, embedding, max_len, n_words = seq_to_ids_and_embeds(seqs, embedding_dim = embedding_dim)\n",
    "keras_y = keras.utils.np_utils.to_categorical(y)\n",
    "\n",
    "keras_train_y, keras_test_y = keras_y[:335], keras_y[335:]\n",
    "keras_train_x = pad_sequences(keras_X[:335], maxlen=max_len, dtype='int32')\n",
    "keras_test_x = pad_sequences(keras_X[335:], maxlen=max_len, dtype='int32')\n",
    "\n",
    "\n",
    "model = conv_model_w_dropout(n_words, embedding_dim, embedding, max_len)\n",
    "model.fit(keras_train_x,\n",
    "          keras_train_y, \n",
    "          nb_epoch=5, \n",
    "          validation_data = (keras_test_x, keras_test_y))\n",
    "test_predictions = model.predict(keras_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = map(np.argmax,test_predictions)\n",
    "acts = map(np.argmax, keras_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print classification_report(preds, acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_predictions = model.predict(keras_train_x)\n",
    "preds = map(np.argmax,train_predictions)\n",
    "acts = map(np.argmax, keras_train_y)\n",
    "print classification_report(preds, acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_datascience": {
   "notebookId": 753
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
