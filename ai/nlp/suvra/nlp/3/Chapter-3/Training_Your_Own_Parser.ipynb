{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCannot uninstall requirement networkx, not installed\u001b[0m\n\u001b[31mCannot uninstall requirement pydot, not installed\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n  fonts-liberation libcdt5 libcgraph6 libgd3 libgvc6 libgvpr2 libltdl7\n  libpathplan4 libvpx3\nUse 'sudo apt autoremove' to remove them.\nThe following packages will be REMOVED:\n  graphviz*\n0 upgraded, 0 newly installed, 1 to remove and 70 not upgraded.\nAfter this operation, 9,736 kB disk space will be freed.\n(Reading database ... 29552 files and directories currently installed.)\nRemoving graphviz (2.38.0-12ubuntu2.1) ...\nPurging configuration files for graphviz (2.38.0-12ubuntu2.1) ...\nProcessing triggers for man-db (2.7.5-1) ...\nUninstalling pyparsing-2.1.10:\n  Successfully uninstalled pyparsing-2.1.10\nCollecting networkx\n  Using cached networkx-1.11-py2.py3-none-any.whl\nRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from networkx)\nInstalling collected packages: networkx\nSuccessfully installed networkx-1.11\nCollecting pydotplus\nRequirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from pydotplus)\nInstalling collected packages: pydotplus\nSuccessfully installed pydotplus-2.0.2\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nSuggested packages:\n  gsfonts graphviz-doc\nThe following NEW packages will be installed:\n  graphviz\n0 upgraded, 1 newly installed, 0 to remove and 70 not upgraded.\nNeed to get 680 kB of archives.\nAfter this operation, 9,736 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 graphviz amd64 2.38.0-12ubuntu2.1 [680 kB]\nFetched 680 kB in 1s (602 kB/s)\ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package graphviz.\n(Reading database ... 29452 files and directories currently installed.)\nPreparing to unpack .../graphviz_2.38.0-12ubuntu2.1_amd64.deb ...\nUnpacking graphviz (2.38.0-12ubuntu2.1) ...\nProcessing triggers for man-db (2.7.5-1) ...\nSetting up graphviz (2.38.0-12ubuntu2.1) ...\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "def get_materials(spacy = False, networkx = False):\n",
    "    \n",
    "    if networkx:\n",
    "        !pip uninstall networkx -yy\n",
    "        !pip uninstall pydot -yy\n",
    "        !sudo apt-get purge graphviz -yy\n",
    "        !pip uninstall pyparsing -yy\n",
    "        !pip install networkx\n",
    "        !pip install pydotplus\n",
    "        !sudo apt-get install graphviz -yy\n",
    "        \n",
    "    if spacy:   \n",
    "        !pip install spacy\n",
    "        !python -m spacy.en.download\n",
    "        \n",
    "#import sys\n",
    "#sys.path.append('/home/jupyter/site-packages/')\n",
    "from IPython.display import HTML\n",
    "get_materials(spacy=True, networkx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import spacy.util\n",
    "from spacy.en import English\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.syntax.util import Config\n",
    "from spacy.syntax.arc_eager import ArcEager\n",
    "from spacy.syntax.parser import Parser\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.pipeline import DependencyParser\n",
    "from spacy.syntax.nonproj import PseudoProjectivity\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.symbols import PUNCT, SYM, VERB, DET, ADJ, NOUN, ADP, INTJ, X, PART, PRON, ADV, PROPN, CONJ\n",
    "from spacy.attrs import POS\n",
    "\n",
    "def prepare_environment(path_to_model):\n",
    "    \n",
    "    model_path = Path(path_to_model)\n",
    "    if not model_path.exists():\n",
    "        model_path.mkdir()\n",
    "        \n",
    "    if not (model_path / \"deps\").exists():\n",
    "        (model_path / \"deps\").mkdir()\n",
    "        \n",
    "    if not (model_path / \"pos\").exists():\n",
    "        (model_path / \"pos\").mkdir()             \n",
    "            \n",
    "def _parse_line(line):\n",
    "    pieces = line.split()\n",
    "    id_ = int(pieces[0].split('_')[-1])\n",
    "    word = pieces[1]\n",
    "    pos = pieces[4]\n",
    "    head_idx = int(pieces[6])-1\n",
    "    label = pieces[7]\n",
    "    return word, pos, head_idx, label\n",
    "\n",
    "\n",
    "def read_conll_from_path(path):\n",
    "    \"\"\"Read a standard CoNLL/MALT-style format\"\"\"\n",
    "    with io.open(path, 'r', encoding='utf8') as file_:\n",
    "        sents = []\n",
    "        for sent_str in file_.read().strip().split('\\n\\n'):\n",
    "            ids = []\n",
    "            words = []\n",
    "            heads = []\n",
    "            labels = []\n",
    "            tags = []\n",
    "            for i, line in enumerate(sent_str.split('\\n')):\n",
    "                word, pos_string, head_idx, label = _parse_line(line)\n",
    "                words.append(word)\n",
    "                if head_idx < 0:\n",
    "                    head_idx = i\n",
    "                ids.append(i)\n",
    "                heads.append(head_idx)\n",
    "                labels.append(label)\n",
    "                tags.append(pos_string)\n",
    "            text = ' '.join(words)\n",
    "            annot = (ids, words, tags, heads, labels, ['O'] * len(ids))\n",
    "            sents.append((text, [(annot, [])]))\n",
    "    return sents\n",
    "\n",
    "def process_data(data):\n",
    "    return PseudoProjectivity.preprocess_training_data(data)\n",
    "    \n",
    "    \n",
    "def write_config_file(model_dir, features, actions):\n",
    "    model_path = Path(model_dir)\n",
    "    with (model_path / 'deps' / 'config.json').open('wb') as file_:\n",
    "        file_.write(\n",
    "            json.dumps(\n",
    "                {'pseudoprojective': True, 'features': features, 'labels':actions}).encode('utf8'))\n",
    "def get_label_set(gold_tuples):\n",
    "    left_labels, right_labels = [set()] * 2\n",
    "    for _, sents in gold_tuples:\n",
    "        for annot_tuples, _ in sents:\n",
    "            _, words, deps, heads, thing1, thing2 = annot_tuples\n",
    "            for i, (head, dep) in enumerate(zip(heads, deps)):\n",
    "                if i < head:\n",
    "                    left_labels.add(dep)\n",
    "                elif i > head:\n",
    "                    right_labels.add(dep)   \n",
    "    return sorted(left_labels), sorted(right_labels)\n",
    "\n",
    "\n",
    "def create_language_model(model_dir, features, train_sents, actions):\n",
    "    \n",
    "    left_labels, right_labels = get_label_set(train_sents)\n",
    "    \n",
    "    tag_map = English.Defaults.tag_map\n",
    "    \n",
    "    additional_tags = {\n",
    "        u'~':{POS:X},\n",
    "        u'@':{POS:X},\n",
    "        u'D':{POS: DET},\n",
    "        u'A':{POS: ADJ}, \n",
    "        u'N':{POS: NOUN},  \n",
    "        u'P':{POS: ADP},      \n",
    "        u'!':{POS: INTJ},  \n",
    "        u'#':{POS: X},   \n",
    "        u',':{POS: PUNCT},    \n",
    "        u'S':{POS: NOUN, u'Poss': u'yes'}, \n",
    "        u'Z':{POS: PROPN, u'Poss': u'yes'},\n",
    "        u'O':{POS: PRON},\n",
    "        u'R':{POS: ADV},        \n",
    "        u'V':{POS:VERB},\n",
    "        u'E':{POS: X},\n",
    "        u'G':{POS: X},        \n",
    "        u'^':{POS: PROPN},   \n",
    "        u'&':{POS: CONJ},        \n",
    "        u'L':{POS: VERB},  \n",
    "        u'U':{POS: X},\n",
    "        u'X':{POS: ADJ},\n",
    "        u'T':{POS: PART},\n",
    "        u'Y':{POS: VERB},\n",
    "        u'M':{POS: VERB, u'NounType': u'prop'},\n",
    "    }\n",
    "    tag_map.update(additional_tags)\n",
    "    \n",
    "    vocab = Vocab(lex_attr_getters=Language.Defaults.lex_attr_getters, tag_map=tag_map)\n",
    "    tagger = Tagger(vocab, tag_map=tag_map)\n",
    "    parser = DependencyParser(vocab, features = features, left_labels=left_labels, right_labels=right_labels, actions = actions)    \n",
    "    nlp = Language(vocab=vocab, tagger = tagger, parser = parser)\n",
    "    \n",
    "    return nlp\n",
    "    \n",
    "          \n",
    "\n",
    "\n",
    "def score_model(nlp, gold_docs, verbose=False):\n",
    "    scorer = Scorer()\n",
    "    for _, gold_doc in gold_docs:\n",
    "        for (ids, words, pos, dep_idx, dep_relations, entities), _ in gold_doc:\n",
    "            doc = Doc(nlp.vocab, words=words)\n",
    "            nlp.tagger(doc)\n",
    "            nlp.parser(doc)\n",
    "            PseudoProjectivity.deprojectivize(doc)\n",
    "            gold = GoldParse(doc, tags=pos, heads=dep_idx, deps=dep_relations)\n",
    "            scorer.score(doc, gold, verbose=verbose)\n",
    "    return scorer\n",
    "\n",
    "\n",
    "def train(nlp, training_sents,testing_sents, n_iter = 20):\n",
    "\n",
    "    print(\"Itn.\\t\\tPOS% \\tUAS% \\tLAS%\")\n",
    "    scorer = score_model(nlp, testing_sents)\n",
    "    print('%s:\\t%d\\t%.3f\\t%.3f\\t' % (\"Pretrain\", scorer.tags_acc, scorer.uas,scorer.las))\n",
    "    \n",
    "    for itn in range(n_iter): \n",
    "        loss = 0\n",
    "        for _, sents in training_sents:\n",
    "            for (ids, words, pos, dep_idx, dep_relations, entities), _ in sents:\n",
    "                doc = nlp.tokenizer.tokens_from_list(words)\n",
    "                gold = GoldParse(doc, annot_tuples=None, words=words, tags=pos, heads=dep_idx, deps=dep_relations, make_projective=False)                          \n",
    "                nlp.tagger.update(doc, gold)\n",
    "                nlp.tagger(doc)\n",
    "                nlp.parser.update(doc, gold) \n",
    "                \n",
    "        scorer = score_model(nlp, testing_sents)\n",
    "\n",
    "        random.shuffle(training_sents)\n",
    "        print('%d:\\t\\t%d\\t%.3f\\t%.3f\\t' % (itn, scorer.tags_acc, scorer.uas,scorer.las))\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Tweebo'...\nremote: Counting objects: 858, done.\u001b[K\nremote: Compressing objects: 100% (643/643), done.\u001b[K\nremote: Total 858 (delta 198), reused 858 (delta 198), pack-reused 0\u001b[K\nReceiving objects: 100% (858/858), 18.89 MiB | 11.40 MiB/s, done.\nResolving deltas: 100% (198/198), done.\nChecking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "def download_data():\n",
    "    if not os.path.exists('Tweebo'):\n",
    "        !git clone https://github.com/SowaLabs/Tweebo.git  \n",
    "    train_path = 'Tweebo/Tweebank/Train_Test_Splited/train'\n",
    "    test_path = 'Tweebo/Tweebank/Train_Test_Splited/test'\n",
    "    return train_path, test_path\n",
    "\n",
    "model_dir = 'custom_model/'\n",
    "path_to_training_data, path_to_testing_data = download_data()\n",
    "\n",
    "\n",
    "train_data = read_conll_from_path(path_to_training_data)\n",
    "train_sents = process_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@senorsarita What's that !\n\nRT @thickinmyhips i hate being employed sometimes !\n\n@monaeebbyy2k11 I miss you too <3 I miss |California| but imma come see you as soon as I come back I #promise\n\nStupid Kilkenny didn't get to meet @Royseven :/\n\nOwning Your Craft in the Fitness Business - http://t.co/hTizU4q9 #Fitness BodyBuilding #Professionals #Vancouver\n\n"
     ]
    }
   ],
   "source": [
    "for i in [8, 18, 43, 46, 56]:\n",
    "    print train_sents[i][0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### CoNLL Format:\n<br>\n* ID: Word index, integer starting at 1 for each new sentence; may be a range for tokens with multiple words.\n* FORM: Word form or punctuation symbol.\n* LEMMA: Lemma or stem of word form.\n* UPOSTAG: Universal part-of-speech tag drawn from our revised version of the Google universal POS tags.\n* XPOSTAG: Language-specific part-of-speech tag; underscore if not available.\n* FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n* HEAD: Head of the current token, which is either a value of ID or zero (0).\n* DEPREL: Universal Stanford dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n* DEPS: List of secondary dependencies (head-deprel pairs).\n* MISC: Any other annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def train_tagger_and_parser_model(model_dir, path_to_training_data, path_to_testing_data):\n",
    "    \n",
    "    prepare_environment(model_dir)\n",
    "    train_data = read_conll_from_path(path_to_training_data)\n",
    "    train_sents = process_data(train_data)\n",
    "\n",
    "    test_data = read_conll_from_path(path_to_testing_data)\n",
    "    test_sents = process_data(test_data)\n",
    "\n",
    "    actions = ArcEager.get_actions(gold_parses=train_sents)\n",
    "    features = Language.Defaults.parser_features\n",
    "    write_config_file(model_dir, features, actions)\n",
    "\n",
    "    nlp = create_language_model(model_dir, features, train_sents, actions)\n",
    "    nlp = train(nlp, train_sents, test_sents)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itn.\t\tPOS% \tUAS% \tLAS%\nPretrain:\t2\t26.488\t5.706\t\n0:\t\t71\t67.172\t63.966\t\n1:\t\t75\t65.868\t62.416\t\n2:\t\t75\t69.215\t65.868\t\n3:\t\t75\t71.786\t67.524\t\n4:\t\t78\t71.786\t68.897\t\n5:\t\t81\t73.300\t69.602\t\n6:\t\t78\t75.308\t72.032\t\n7:\t\t81\t75.238\t72.032\t\n8:\t\t80\t74.604\t72.068\t\n9:\t\t80\t74.111\t70.940\t\n10:\t\t80\t73.124\t69.179\t\n11:\t\t81\t77.316\t72.385\t\n12:\t\t81\t76.682\t72.702\t\n13:\t\t81\t78.514\t74.921\t\n14:\t\t81\t77.527\t75.167\t\n15:\t\t82\t76.893\t73.864\t\n16:\t\t81\t76.964\t73.441\t\n17:\t\t81\t76.541\t73.160\t\n18:\t\t80\t75.343\t72.068\t\n19:\t\t81\t75.273\t72.138\t\n"
     ]
    }
   ],
   "source": [
    "nlp = train_tagger_and_parser_model(model_dir, path_to_training_data, path_to_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "_datascience": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'RT', u'X', u'~', u'ROOT'),\n (u':', u'PUNCT', u',', u'ROOT'),\n (u'@MikeyMike', u'X', u'@', u'ROOT'),\n (u'I', u'PRON', u'O', u'_'),\n (u'went', u'VERB', u'V', u'ROOT'),\n (u'to', u'ADP', u'P', u'_'),\n (u'the', u'DET', u'D', u'MWE'),\n (u'#', u'PUNCT', u',', u'ROOT'),\n (u'KanyeWest', u'PROPN', u'^', u'ROOT'),\n (u'concert', u'NOUN', u'N', u'ROOT'),\n (u'too', u'ADV', u'R', u'_'),\n (u'!', u'PUNCT', u',', u'ROOT')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = u'RT: @MikeyMike I went to the #KanyeWest concert too!'\n",
    "doc = nlp(tweet)\n",
    "map(lambda x: (x.orth_, x.pos_, x.tag_, x.dep_), doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "pygments_lexer": "ipython2",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "2.7.12",
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python"
  },
  "_datascience": {
   "notebookId": 669.0
  },
  "kernelspec": {
   "language": "python",
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
