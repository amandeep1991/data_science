# Categorical Features
## Tempted to encode this data with a straightforward numerical mapping
>* {'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};
>>* Not generally a useful approach, because this makes this value orderable which is not actually the case.

## Proven technique is to use one-hot encoding
>* When your data comes as a list of dictionaries, Scikit-Learn's DictVectorizer will do this for you. 
```python
data = [
    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},
    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},
    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},
    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}
]


from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer(sparse=False, dtype=int)
vec.fit_transform(data)
#array([[     0,      1,      0, 850000,      4],
#       [     1,      0,      0, 700000,      3],
#       [     0,      0,      1, 650000,      3],
#       [     1,      0,      0, 600000,      2]], dtype=int64)
       

vec.get_feature_names()
#['neighborhood=Fremont',
# 'neighborhood=Queen Anne',
# 'neighborhood=Wallingford',
# 'price',
# 'rooms']

``` 

>* Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.
>* There is one clear disadvantage of this approach: if your category has many possible values, this can greatly increase the size of your dataset. 
>* **However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution**:
```python
vec = DictVectorizer(sparse=True, dtype=int)
vec.fit_transform(data)
#<4x5 sparse matrix of type '<class 'numpy.int64'>'
#	with 12 stored elements in Compressed Sparse Row format>

```
>>* sklearn.preprocessing.OneHotEncoder and sklearn.feature_extraction.FeatureHasher are two additional tools.


# Text Features:
>* Another common need in feature engineering is to convert text to a set of representative numerical values. For example, most automatic mining of social media data relies on some form of encoding the text as numbers. One of the simplest methods of encoding data is by word counts: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.

```python
sample = ['problem of evil',
          'evil queen',
          'horizon problem']
          
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(sample)
X
#<3x5 sparse matrix of type '<class 'numpy.int64'>'
#	with 7 stored elements in Compressed Sparse Row format>


import pandas as pd
pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
```

| in|  evil|horizon|of	|problem|queen|
|---|------|-------|----|-------|-----|
|0  |1	   |0	   |1	|    1	|0    |
|1  |1	   |0	   |0	|    0	|1    |
|2  |0	   |1	   |0	|    1	|0    |

>* There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms. One approach to fix this is known as term frequency-inverse document frequency (TFâ€“IDF) which weights the word counts by a measure of how often they appear in the documents. The syntax for computing these features is similar to the previous example:

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer()
X = vec.fit_transform(sample)
pd.DataFrame(X.toarray(), columns=vec.get_feature_names())

```

| . |  evil	|horizon|of	|problem| queen|
|---|------|------|------|---------|------|
|0	|0.517856	|0.000000	|0.680919	|0.517856	|0.000000|
|1	|0.605349	|0.000000	|0.000000	|0.000000	|0.795961|
|2	|0.000000	|0.795961	|0.000000	|0.605349	|0.000000|

# Image Features
>* Image processing in Python : (scikit-image)


# Derived Features
>* Another useful type of feature is one that is mathematically derived from some input features. 


# Imputation of Missing Data
>* When applying a typical machine learning model to following data, we will need to first replace such missing data with some appropriate fill value. This is known as imputation of missing values.
```python
import numpy as np
from numpy import nan
X = np.array([[ nan, 0,   3  ],
              [ 3,   7,   9  ],
              [ 3,   5,   2  ],
              [ 4,   nan, 6  ],
              [ 8,   8,   1  ]])
y = np.array([14, 16, -1,  8, -5])




from sklearn.preprocessing import Imputer
imp = Imputer(strategy='mean')
X2 = imp.fit_transform(X)
X2

```


# Feature Pipelines
>* With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps. 
>* For example - inputing missing values using mean, transform features to quadratic, fit a linear regression.
```python
from sklearn.pipeline import make_pipeline

model = make_pipeline(Imputer(strategy='mean'),
                      PolynomialFeatures(degree=2),
                      LinearRegression())
```